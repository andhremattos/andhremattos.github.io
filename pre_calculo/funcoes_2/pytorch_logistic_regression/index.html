<!doctype html><html lang=pt class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Plataforma para disponibilização cursos de cálculo."><link href=https:///disponha.com/pre_calculo/funcoes_2/pytorch_logistic_regression/ rel=canonical><meta name=author content="Carlos André"><link rel="shortcut icon" href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.1.2, mkdocs-material-5.4.0"><title>Logistic Regression with PyTorch - disponha</title><link rel=stylesheet href=../../../assets/stylesheets/main.545621a7.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.36d1b78f.min.css><meta name=theme-color content=#009688><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Helvetica:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Helvetica",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","None","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=teal data-md-color-accent=teal> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#logistic-regression-with-pytorch class=md-skip> Ir para o conteúdo </a> </div> <!-- 
    <div data-md-component="announce">
      
    </div>
  --> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https:/disponha.com/ title=disponha class="md-header-nav__button md-logo" aria-label=disponha> <?xml version="1.0" encoding="UTF-8"?> <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><!-- Creator: CorelDRAW X7 --><svg xmlns=http://www.w3.org/2000/svg xml:space=preserve width=9000px height=9000px version=1.1 style="shape-rendering:geometricPrecision; text-rendering:geometricPrecision; image-rendering:optimizeQuality; fill-rule:evenodd; clip-rule:evenodd" viewbox="0 0 9000 9000" xmlns:xlink=http://www.w3.org/1999/xlink> <defs> <style type=text/css>
   <![CDATA[
    .str0 {stroke:#2E72A4;stroke-width:118.11}
    .fil0 {fill:#FEFEFE}
    .fil1 {fill:#2E72A4}
   ]]>
  </style> </defs> <g id=Camada_x0020_1> <metadata id=CorelCorpID_0Corel-Layer /> <g id=_1793748728448> <circle cx=4472 cy=4507 r=4281 class="fil0 str0"/> <path class="fil1 str0" d="M2045 5171c120,204 210,379 402,449 314,115 1000,-177 1339,-330 79,-35 179,-68 254,-120 41,-13 94,-46 135,-68l951 -479c363,-177 729,-363 1095,-521 186,-80 370,-180 558,-244 465,-158 1366,-413 1814,-369l1 -59c-328,-34 -853,93 -1189,181 -431,113 -680,200 -1103,395 -706,324 -1401,670 -2099,1021 -397,199 -1256,618 -1669,556 -218,-33 -320,-195 -449,-416 -47,-80 -137,-241 -177,-328 -49,-108 -114,-212 -179,-311 -379,-575 -831,-826 -1274,-1083l-119 -63 -10 46 247 149c311,184 648,381 929,716 107,128 198,250 291,408 61,104 217,424 252,470z"/> </g> </g> </svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> disponha </span> <span class="md-header-nav__topic md-ellipsis"> Logistic Regression with PyTorch </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Buscar placeholder=Buscar autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <!-- 

<a href="https://github.com/andhremattos/" title="Ir ao repositório" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    esead
  </div>
</a>
--> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <!--
      <a href="../../.." class="md-tabs__link md-tabs__link--active">
        Home
      </a>
    --> <a href=/explore/ class="md-tabs__link md-tabs__link--active"> Explore e-books </a> </li> <!--
    <li class="md-tabs__item">
      
        <a href="../../../sobre/" class="md-tabs__link">
          Sobre
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../explore/" class="md-tabs__link">
          Explore
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../app_web_ebook/servico/" class="md-tabs__link">
          App Web ebook
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../diagramacao/servico/" class="md-tabs__link">
          Diagramação
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../ciencia_de_dados/servico/" class="md-tabs__link">
          Ciência de dados
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../aprendacalculo/" class="md-tabs__link">
          Aprenda Cálculo
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../cursolatex/" class="md-tabs__link">
          Curso de LaTeX
        </a>
      
    </li>--> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https:/disponha.com/ title=disponha class="md-nav__button md-logo" aria-label=disponha> <?xml version="1.0" encoding="UTF-8"?> <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><!-- Creator: CorelDRAW X7 --><svg xmlns=http://www.w3.org/2000/svg xml:space=preserve width=9000px height=9000px version=1.1 style="shape-rendering:geometricPrecision; text-rendering:geometricPrecision; image-rendering:optimizeQuality; fill-rule:evenodd; clip-rule:evenodd" viewbox="0 0 9000 9000" xmlns:xlink=http://www.w3.org/1999/xlink> <defs> <style type=text/css>
   <![CDATA[
    .str0 {stroke:#2E72A4;stroke-width:118.11}
    .fil0 {fill:#FEFEFE}
    .fil1 {fill:#2E72A4}
   ]]>
  </style> </defs> <g id=Camada_x0020_1> <metadata id=CorelCorpID_0Corel-Layer /> <g id=_1793748728448> <circle cx=4472 cy=4507 r=4281 class="fil0 str0"/> <path class="fil1 str0" d="M2045 5171c120,204 210,379 402,449 314,115 1000,-177 1339,-330 79,-35 179,-68 254,-120 41,-13 94,-46 135,-68l951 -479c363,-177 729,-363 1095,-521 186,-80 370,-180 558,-244 465,-158 1366,-413 1814,-369l1 -59c-328,-34 -853,93 -1189,181 -431,113 -680,200 -1103,395 -706,324 -1401,670 -2099,1021 -397,199 -1256,618 -1669,556 -218,-33 -320,-195 -449,-416 -47,-80 -137,-241 -177,-328 -49,-108 -114,-212 -179,-311 -379,-575 -831,-826 -1274,-1083l-119 -63 -10 46 247 149c311,184 648,381 929,716 107,128 198,250 291,408 61,104 217,424 252,470z"/> </g> </g> </svg> </a> disponha </label> <div class=md-nav__source> <!-- 

<a href="https://github.com/andhremattos/" title="Ir ao repositório" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    esead
  </div>
</a>
--> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. title=Home class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2> <label class=md-nav__link for=nav-2> Sobre <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Sobre data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Sobre </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../sobre/ title=Home class=md-nav__link> Home </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Explore <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Explore data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Explore </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../explore/ title=E-books class=md-nav__link> E-books </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../apoie/ title=Apoie class=md-nav__link> Apoie </a> </li> <li class=md-nav__item> <a href=../../../consultoria/ title=Consultoria class=md-nav__link> Consultoria </a> </li> <li class=md-nav__item> <a href=../../../politica_privacidade/ title=Privacidade class=md-nav__link> Privacidade </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7 type=checkbox id=nav-7> <label class=md-nav__link for=nav-7> App Web ebook <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="App Web ebook" data-md-level=1> <label class=md-nav__title for=nav-7> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> App Web ebook </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../app_web_ebook/servico/ title=Serviço class=md-nav__link> Serviço </a> </li> <li class=md-nav__item> <a href=../../../app_web_ebook/amostra/ title="App Web ebook - amostra" class=md-nav__link> App Web ebook - amostra </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-8 type=checkbox id=nav-8> <label class=md-nav__link for=nav-8> Diagramação <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Diagramação data-md-level=1> <label class=md-nav__title for=nav-8> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Diagramação </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../diagramacao/servico/ title=Serviço class=md-nav__link> Serviço </a> </li> <li class=md-nav__item> <a href=../../../diagramacao/amostra-diagramacao/ title=Amostra class=md-nav__link> Amostra </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-9 type=checkbox id=nav-9> <label class=md-nav__link for=nav-9> Ciência de dados <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Ciência de dados" data-md-level=1> <label class=md-nav__title for=nav-9> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Ciência de dados </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../ciencia_de_dados/servico/ title=Serviço class=md-nav__link> Serviço </a> </li> <li class=md-nav__item> <a href=../../../ciencia_de_dados/amostra-1/ title="Amostra 1" class=md-nav__link> Amostra 1 </a> </li> <li class=md-nav__item> <a href=../../../ciencia_de_dados/amostra-2/ title="Amostra 2" class=md-nav__link> Amostra 2 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-10 type=checkbox id=nav-10> <label class=md-nav__link for=nav-10> Aprenda Cálculo <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Aprenda Cálculo" data-md-level=1> <label class=md-nav__title for=nav-10> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Aprenda Cálculo </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../aprendacalculo/ title=Bem-Vindo(a)s class=md-nav__link> Bem-Vindo(a)s </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/pre_calculo/ title=Pré-Cálculo class=md-nav__link> Pré-Cálculo </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_i/ title="Cálculo I" class=md-nav__link> Cálculo I </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_ii/ title="Cálculo II" class=md-nav__link> Cálculo II </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_iii/ title="Cálculo III" class=md-nav__link> Cálculo III </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_iv/ title="Cálculo IV" class=md-nav__link> Cálculo IV </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_numerico/ title="C. Numérico" class=md-nav__link> C. Numérico </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-11 type=checkbox id=nav-11> <label class=md-nav__link for=nav-11> Curso de LaTeX <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Curso de LaTeX" data-md-level=1> <label class=md-nav__title for=nav-11> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Curso de LaTeX </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../cursolatex/ title=Bem-Vindo(a)s class=md-nav__link> Bem-Vindo(a)s </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/programas/ title=Programas class=md-nav__link> Programas </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/latex_online/ title="LaTeX online" class=md-nav__link> LaTeX online </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_1/ title="Parte 1" class=md-nav__link> Parte 1 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_2/ title="Parte 2" class=md-nav__link> Parte 2 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_3/ title="Parte 3" class=md-nav__link> Parte 3 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_4/ title="Parte 4" class=md-nav__link> Parte 4 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_5/ title="Parte 5" class=md-nav__link> Parte 5 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_6/ title="Parte 6" class=md-nav__link> Parte 6 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_7/ title="Parte 7" class=md-nav__link> Parte 7 </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=Índice> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Índice </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#about-logistic-regression class=md-nav__link> About Logistic Regression </a> <nav class=md-nav aria-label="About Logistic Regression"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#logistic-regression-basics class=md-nav__link> Logistic Regression Basics </a> <nav class=md-nav aria-label="Logistic Regression Basics"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#classification-algorithm class=md-nav__link> Classification algorithm </a> </li> <li class=md-nav__item> <a href=#basic-comparison class=md-nav__link> Basic Comparison </a> </li> <li class=md-nav__item> <a href=#inputoutput-comparison class=md-nav__link> Input/Output Comparison </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#problems-of-linear-regression class=md-nav__link> Problems of Linear Regression </a> </li> <li class=md-nav__item> <a href=#logistic-regression-in-depth class=md-nav__link> Logistic Regression In-Depth </a> <nav class=md-nav aria-label="Logistic Regression In-Depth"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#predicting-probability class=md-nav__link> Predicting Probability </a> </li> <li class=md-nav__item> <a href=#logistic-function-g class=md-nav__link> Logistic Function g() </a> </li> <li class=md-nav__item> <a href=#softmax-function-g class=md-nav__link> Softmax Function g() </a> </li> <li class=md-nav__item> <a href=#cross-entropy-function-d-for-2-class class=md-nav__link> Cross Entropy Function D() for 2 Class </a> </li> <li class=md-nav__item> <a href=#cross-entropy-function-d-for-more-than-2-class class=md-nav__link> Cross Entropy Function D() for More Than 2 Class </a> </li> <li class=md-nav__item> <a href=#cross-entropy-loss-over-n-samples class=md-nav__link> Cross Entropy Loss over N samples </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#building-a-logistic-regression-model-with-pytorch class=md-nav__link> Building a Logistic Regression Model with PyTorch </a> <nav class=md-nav aria-label="Building a Logistic Regression Model with PyTorch"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#steps class=md-nav__link> Steps </a> </li> <li class=md-nav__item> <a href=#step-1a-loading-mnist-train-dataset class=md-nav__link> Step 1a: Loading MNIST Train Dataset </a> <nav class=md-nav aria-label="Step 1a: Loading MNIST Train Dataset"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#displaying-mnist class=md-nav__link> Displaying MNIST </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#step-1b-loading-mnist-test-dataset class=md-nav__link> Step 1b: Loading MNIST Test Dataset </a> </li> <li class=md-nav__item> <a href=#step-2-make-dataset-iterable class=md-nav__link> Step 2: Make Dataset Iterable </a> </li> <li class=md-nav__item> <a href=#step-3-building-model class=md-nav__link> Step 3: Building Model </a> </li> <li class=md-nav__item> <a href=#step-4-instantiate-model-class class=md-nav__link> Step 4: Instantiate Model Class </a> </li> <li class=md-nav__item> <a href=#step-5-instantiate-loss-class class=md-nav__link> Step 5: Instantiate Loss Class </a> </li> <li class=md-nav__item> <a href=#step-6-instantiate-optimizer-class class=md-nav__link> Step 6: Instantiate Optimizer Class </a> </li> <li class=md-nav__item> <a href=#step-7-train-model class=md-nav__link> Step 7: Train Model </a> <nav class=md-nav aria-label="Step 7: Train Model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#break-down-accuracy-calculation class=md-nav__link> Break Down Accuracy Calculation </a> </li> <li class=md-nav__item> <a href=#saving-model class=md-nav__link> Saving Model </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#building-a-logistic-regression-model-with-pytorch-gpu class=md-nav__link> Building a Logistic Regression Model with PyTorch (GPU) </a> </li> <li class=md-nav__item> <a href=#summary class=md-nav__link> Summary </a> </li> <li class=md-nav__item> <a href=#citation class=md-nav__link> Citation </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=logistic-regression-with-pytorch>Logistic Regression with PyTorch<a class=headerlink href=#logistic-regression-with-pytorch title="Permanent link">&para;</a></h1> <div class="admonition tip"> <p class=admonition-title>Run Jupyter Notebook</p> <p>You can run the code for this section in this <a href=https://github.com/ritchieng/deep-learning-wizard/blob/master/docs/deep_learning/practical_pytorch/pytorch_logistic_regression.ipynb>jupyter notebook link</a>.</p> </div> <h2 id=about-logistic-regression>About Logistic Regression<a class=headerlink href=#about-logistic-regression title="Permanent link">&para;</a></h2> <h3 id=logistic-regression-basics>Logistic Regression Basics<a class=headerlink href=#logistic-regression-basics title="Permanent link">&para;</a></h3> <h4 id=classification-algorithm>Classification algorithm<a class=headerlink href=#classification-algorithm title="Permanent link">&para;</a></h4> <ul> <li>Example: Spam vs No Spam<ul> <li>Input: Bunch of words</li> <li>Output: Probability spam or not</li> </ul> </li> </ul> <h4 id=basic-comparison>Basic Comparison<a class=headerlink href=#basic-comparison title="Permanent link">&para;</a></h4> <ul> <li><strong>Linear regression</strong><ul> <li>Output: numeric value given inputs</li> </ul> </li> <li><strong>Logistic regression</strong>:<ul> <li>Output: probability [0, 1] given input belonging to a class</li> </ul> </li> </ul> <h4 id=inputoutput-comparison>Input/Output Comparison<a class=headerlink href=#inputoutput-comparison title="Permanent link">&para;</a></h4> <ul> <li><strong>Linear regression: Multiplication</strong><ul> <li>Input: [1]<ul> <li>Output: 2</li> </ul> </li> <li>Input: [2]<ul> <li>Output: 4</li> </ul> </li> <li>Trying to model the relationship <code>y = 2x</code></li> </ul> </li> <li><strong>Logistic regression: Spam</strong><ul> <li>Input: "Sign up to get 1 million dollars by tonight"<ul> <li>Output: p = 0.8</li> </ul> </li> <li>Input: "This is a receipt for your recent purchase with Amazon"<ul> <li>Output: p = 0.3</li> </ul> </li> <li><strong>p: probability it is spam</strong></li> </ul> </li> </ul> <h3 id=problems-of-linear-regression>Problems of Linear Regression<a class=headerlink href=#problems-of-linear-regression title="Permanent link">&para;</a></h3> <ul> <li>Example<ul> <li>Fever</li> <li><strong>Input</strong>: temperature</li> <li><strong>Output</strong>: fever or no fever</li> </ul> </li> <li>Remember<ul> <li><strong>Linear regression</strong>: minimize error between points and line</li> </ul> </li> </ul> <div class="admonition bug"> <p class=admonition-title>Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1)</p> <p>If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context. <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=o>%</span><span class=n>matplotlib</span> <span class=n>inline</span>

<span class=n>x</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>25</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>70</span><span class=p>,</span> <span class=mi>75</span><span class=p>,</span> <span class=mi>100</span><span class=p>,]</span>
<span class=n>y</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>

<span class=n>colors</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>poly1d</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>polyfit</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=mi>1</span><span class=p>))(</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;Fever&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Temperature&quot;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=n>colors</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div></p> </div> <p><img alt=png src=../pytorch_logistic_regression_files/pytorch_logistic_regression_5_1.png></p> <div class="admonition bug"> <p class=admonition-title>Linear Regression Problem 2: Fever points are not predicted with the presence of outliers</p> <p>Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all. <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>

<span class=n>x</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>25</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>70</span><span class=p>,</span> <span class=mi>75</span><span class=p>,</span> <span class=mi>300</span><span class=p>]</span>
<span class=n>y</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>

<span class=n>colors</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>poly1d</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>polyfit</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=mi>1</span><span class=p>))(</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;Fever&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Temperature&quot;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=n>colors</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div></p> </div> <p><img alt=png src=../pytorch_logistic_regression_files/pytorch_logistic_regression_5_0.png></p> <h3 id=logistic-regression-in-depth>Logistic Regression In-Depth<a class=headerlink href=#logistic-regression-in-depth title="Permanent link">&para;</a></h3> <h4 id=predicting-probability>Predicting Probability<a class=headerlink href=#predicting-probability title="Permanent link">&para;</a></h4> <ul> <li>Linear regression doesn't work</li> <li>Instead of predicting direct values: <strong>predict probability</strong></li> </ul> <p><img alt src=../images/cross_entropy_final_4.png></p> <h4 id=logistic-function-g>Logistic Function g()<a class=headerlink href=#logistic-function-g title="Permanent link">&para;</a></h4> <ul> <li><strong>"Two-class logistic regression"</strong></li> <li><span><span class=MathJax_Preview>\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</span><script type=math/tex>\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</script></span><ul> <li>Where <span><span class=MathJax_Preview>\boldsymbol{y}</span><script type=math/tex>\boldsymbol{y}</script></span> is a vector comprising the 2-class prediction <span><span class=MathJax_Preview>y_0</span><script type=math/tex>y_0</script></span> and <span><span class=MathJax_Preview>y_1</span><script type=math/tex>y_1</script></span></li> <li>Where the labels are <span><span class=MathJax_Preview>y_0 = 0</span><script type=math/tex>y_0 = 0</script></span> and <span><span class=MathJax_Preview>y_1 = 1</span><script type=math/tex>y_1 = 1</script></span></li> <li>Also, it's bolded because it's a vector, not a matrix.</li> </ul> </li> <li><span><span class=MathJax_Preview>g(y_1) = \frac {1} {1 + e^{-y_1}}</span><script type=math/tex>g(y_1) = \frac {1} {1 + e^{-y_1}}</script></span><ul> <li><span><span class=MathJax_Preview>g(y_1)</span><script type=math/tex>g(y_1)</script></span> = Estimated probability that <span><span class=MathJax_Preview>y = 1</span><script type=math/tex>y = 1</script></span></li> </ul> </li> <li><span><span class=MathJax_Preview>g(y_0) = 1 - g(y_1)</span><script type=math/tex>g(y_0) = 1 - g(y_1)</script></span><ul> <li><span><span class=MathJax_Preview>g(y_0)</span><script type=math/tex>g(y_0)</script></span> = Estimated probability that <span><span class=MathJax_Preview>y = 0</span><script type=math/tex>y = 0</script></span></li> </ul> </li> <li>For our illustration above, we have 4 classes, so we have to use softmax function explained below</li> </ul> <h4 id=softmax-function-g>Softmax Function g()<a class=headerlink href=#softmax-function-g title="Permanent link">&para;</a></h4> <ul> <li><strong>"Multi-class logistic regression"</strong><ul> <li>Generalization of logistic function, where you can derive back to the logistic function if you've a 2 class classification problem</li> <li>Here, we will use a 4 class example (K = 4) as shown above to be very clear in how it relates back to that simple examaple.</li> </ul> </li> <li><span><span class=MathJax_Preview>\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</span><script type=math/tex>\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</script></span><ul> <li>Where <span><span class=MathJax_Preview>\boldsymbol{y}</span><script type=math/tex>\boldsymbol{y}</script></span> is a vector comprising the 4-class prediction <span><span class=MathJax_Preview>y_0, y_1, y_2, y_3</span><script type=math/tex>y_0, y_1, y_2, y_3</script></span></li> <li>Where the 4 labels (K = 4) are <span><span class=MathJax_Preview>y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3</span><script type=math/tex>y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3</script></span></li> </ul> </li> <li><span><span class=MathJax_Preview>g(y_i) = \frac {e^{y_i} } {\sum^K_i e^{y_i}}</span><script type=math/tex>g(y_i) = \frac {e^{y_i} } {\sum^K_i e^{y_i}}</script></span> where K = 4 because we have 4 classes<ul> <li>To put numbers to this equation in relation to the illustration above where we've <span><span class=MathJax_Preview>y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8</span><script type=math/tex>y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8</script></span><ul> <li><span><span class=MathJax_Preview>g(y_0) = \frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017</span><script type=math/tex>g(y_0) = \frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017</script></span></li> <li><span><span class=MathJax_Preview>g(y_1) = \frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015</span><script type=math/tex>g(y_1) = \frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015</script></span></li> <li><span><span class=MathJax_Preview>g(y_2) = \frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412</span><script type=math/tex>g(y_2) = \frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412</script></span></li> <li><span><span class=MathJax_Preview>g(y_3) = \frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556</span><script type=math/tex>g(y_3) = \frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556</script></span></li> <li><span><span class=MathJax_Preview>g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0</span><script type=math/tex>g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0</script></span></li> <li>All softmax outputs have to sum to one as they represent a probability distribution over K classes. </li> </ul> </li> </ul> </li> <li>Take note how these numbers are not exactly as in the illustration in the softmax box but the concept is important (intentionally made so).<ul> <li><span><span class=MathJax_Preview>y_0</span><script type=math/tex>y_0</script></span> and <span><span class=MathJax_Preview>y_1</span><script type=math/tex>y_1</script></span> are approximately similar in values and they return similar probabilities.</li> <li>Similarly, <span><span class=MathJax_Preview>y_2</span><script type=math/tex>y_2</script></span> and <span><span class=MathJax_Preview>y_3</span><script type=math/tex>y_3</script></span> are approximately similar in values and they return similar probabilities.</li> </ul> </li> </ul> <div class="admonition bug"> <p class=admonition-title>Softmax versus Soft(arg)max</p> <p>Do you know many researchers and anyone in deep learning in general use the term softmax when it should be soft(arg)max.</p> <p>This is because soft(arg)max returns the probability distribution over K classes, a vector. </p> <p>However, softmax only returns the max! This means you will be getting a scalar value versus a probability distribution.</p> <p>According to my friend, Alfredo Canziani (postdoc in NYU under Yann Lecun), it was actually a mistake made in the original paper previously but it was too late because the term softmax was adopted. Full credits to him for this tip.</p> </div> <h4 id=cross-entropy-function-d-for-2-class>Cross Entropy Function D() for 2 Class<a class=headerlink href=#cross-entropy-function-d-for-2-class title="Permanent link">&para;</a></h4> <ul> <li>Take note that here, <span><span class=MathJax_Preview>S</span><script type=math/tex>S</script></span> is our softmax outputs and <span><span class=MathJax_Preview>L</span><script type=math/tex>L</script></span> are our labels</li> <li><span><span class=MathJax_Preview>D(S, L) = -(L log S + (1-L)log(1-S))</span><script type=math/tex>D(S, L) = -(L log S + (1-L)log(1-S))</script></span><ul> <li>If L = 0 (label)<ul> <li><span><span class=MathJax_Preview>D(S, 0) = - log(1-S)</span><script type=math/tex>D(S, 0) = - log(1-S)</script></span><ul> <li><span><span class=MathJax_Preview>- log(1-S)</span><script type=math/tex>- log(1-S)</script></span>: less positive if <span><span class=MathJax_Preview>S \longrightarrow 0</span><script type=math/tex>S \longrightarrow 0</script></span></li> <li><span><span class=MathJax_Preview>- log(1-S)</span><script type=math/tex>- log(1-S)</script></span>: more positive if <span><span class=MathJax_Preview>S \longrightarrow 1</span><script type=math/tex>S \longrightarrow 1</script></span> (BIGGER LOSS)</li> </ul> </li> </ul> </li> <li>If L = 1 (label)<ul> <li><span><span class=MathJax_Preview>D(S, 1) = - log S</span><script type=math/tex>D(S, 1) = - log S</script></span><ul> <li><span><span class=MathJax_Preview>-log(S)</span><script type=math/tex>-log(S)</script></span>: less positive if <span><span class=MathJax_Preview>S \longrightarrow 1</span><script type=math/tex>S \longrightarrow 1</script></span></li> <li><span><span class=MathJax_Preview>-log(S)</span><script type=math/tex>-log(S)</script></span>: more positive if <span><span class=MathJax_Preview>S \longrightarrow 0</span><script type=math/tex>S \longrightarrow 0</script></span> (BIGGER LOSS)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="admonition note"> <p class=admonition-title>Numerical example of bigger or small loss</p> <p>You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction). <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>math</span>
<span class=nb>print</span><span class=p>(</span><span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=mf>0.00001</span><span class=p>))</span>
</code></pre></div></p> <p>You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction). <div class=highlight><pre><span></span><code><span class=nb>print</span><span class=p>(</span><span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=mf>0.99999</span><span class=p>))</span> 
</code></pre></div></p> <p>You get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction). <div class=highlight><pre><span></span><code><span class=nb>print</span><span class=p>(</span><span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mf>0.99999</span><span class=p>))</span>
</code></pre></div></p> <p>You get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction). <div class=highlight><pre><span></span><code><span class=nb>print</span><span class=p>(</span><span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mf>0.00001</span><span class=p>))</span>
</code></pre></div></p> </div> <div class=highlight><pre><span></span><code><span class=mf>1.0000050000287824e-05</span>
<span class=mf>11.51292546497478</span>
<span class=mf>1.0000050000287824e-05</span>
<span class=mf>11.512925464970229</span>
</code></pre></div> <h4 id=cross-entropy-function-d-for-more-than-2-class>Cross Entropy Function D() for More Than 2 Class<a class=headerlink href=#cross-entropy-function-d-for-more-than-2-class title="Permanent link">&para;</a></h4> <ul> <li>For the case where we have more than 2 class, we need a more generalized function</li> <li><span><span class=MathJax_Preview>D(S, L) = - \sum^K_1 L_i log(S_i)</span><script type=math/tex>D(S, L) = - \sum^K_1 L_i log(S_i)</script></span><ul> <li><span><span class=MathJax_Preview>K</span><script type=math/tex>K</script></span>: number of classes</li> <li><span><span class=MathJax_Preview>L_i</span><script type=math/tex>L_i</script></span>: label of i-th class, 1 if that's the class else 0</li> <li><span><span class=MathJax_Preview>S_i</span><script type=math/tex>S_i</script></span>: output of softmax for i-th class</li> </ul> </li> </ul> <h4 id=cross-entropy-loss-over-n-samples>Cross Entropy Loss over N samples<a class=headerlink href=#cross-entropy-loss-over-n-samples title="Permanent link">&para;</a></h4> <ul> <li>Goal: Minimizing Cross Entropy Loss, L</li> <li><span><span class=MathJax_Preview>Loss = \frac {1}{N} \sum_j^N D_j</span><script type=math/tex>Loss = \frac {1}{N} \sum_j^N D_j</script></span><ul> <li><span><span class=MathJax_Preview>D_j</span><script type=math/tex>D_j</script></span>: j-th sample of cross entropy function <span><span class=MathJax_Preview>D(S, L)</span><script type=math/tex>D(S, L)</script></span></li> <li><span><span class=MathJax_Preview>N</span><script type=math/tex>N</script></span>: number of samples</li> <li><span><span class=MathJax_Preview>Loss</span><script type=math/tex>Loss</script></span>: average cross entropy loss over N samples</li> </ul> </li> </ul> <h2 id=building-a-logistic-regression-model-with-pytorch>Building a Logistic Regression Model with PyTorch<a class=headerlink href=#building-a-logistic-regression-model-with-pytorch title="Permanent link">&para;</a></h2> <p><img alt src=../images/lr2.png></p> <h3 id=steps>Steps<a class=headerlink href=#steps title="Permanent link">&para;</a></h3> <ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <h3 id=step-1a-loading-mnist-train-dataset>Step 1a: Loading MNIST Train Dataset<a class=headerlink href=#step-1a-loading-mnist-train-dataset title="Permanent link">&para;</a></h3> <p><strong>Images from 1 to 9</strong></p> <div class="admonition note"> <p class=admonition-title>Inspect length of training dataset</p> <p>You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images. <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>
</code></pre></div></p> <div class=highlight><pre><span></span><code><span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=mi>60000</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Inspecting a single image</p> <p>So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers. <div class=highlight><pre><span></span><code><span class=n>train_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</code></pre></div></p> </div> <div class=highlight><pre><span></span><code><span class=p>(</span><span class=n>tensor</span><span class=p>([[[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0118</span><span class=p>,</span>  <span class=mf>0.0706</span><span class=p>,</span>
            <span class=mf>0.0706</span><span class=p>,</span>  <span class=mf>0.0706</span><span class=p>,</span>  <span class=mf>0.4941</span><span class=p>,</span>  <span class=mf>0.5333</span><span class=p>,</span>  <span class=mf>0.6863</span><span class=p>,</span>  <span class=mf>0.1020</span><span class=p>,</span>  <span class=mf>0.6510</span><span class=p>,</span>
            <span class=mf>1.0000</span><span class=p>,</span>  <span class=mf>0.9686</span><span class=p>,</span>  <span class=mf>0.4980</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.1176</span><span class=p>,</span>  <span class=mf>0.1412</span><span class=p>,</span>  <span class=mf>0.3686</span><span class=p>,</span>  <span class=mf>0.6039</span><span class=p>,</span>  <span class=mf>0.6667</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>
            <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.8824</span><span class=p>,</span>  <span class=mf>0.6745</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>
            <span class=mf>0.9490</span><span class=p>,</span>  <span class=mf>0.7647</span><span class=p>,</span>  <span class=mf>0.2510</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.1922</span><span class=p>,</span>  <span class=mf>0.9333</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>
            <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9843</span><span class=p>,</span>  <span class=mf>0.3647</span><span class=p>,</span>  <span class=mf>0.3216</span><span class=p>,</span>  <span class=mf>0.3216</span><span class=p>,</span>
            <span class=mf>0.2196</span><span class=p>,</span>  <span class=mf>0.1529</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0706</span><span class=p>,</span>  <span class=mf>0.8588</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>
            <span class=mf>0.7765</span><span class=p>,</span>  <span class=mf>0.7137</span><span class=p>,</span>  <span class=mf>0.9686</span><span class=p>,</span>  <span class=mf>0.9451</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.3137</span><span class=p>,</span>  <span class=mf>0.6118</span><span class=p>,</span>  <span class=mf>0.4196</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.8039</span><span class=p>,</span>
            <span class=mf>0.0431</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.1686</span><span class=p>,</span>  <span class=mf>0.6039</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0549</span><span class=p>,</span>  <span class=mf>0.0039</span><span class=p>,</span>  <span class=mf>0.6039</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.3529</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.5451</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.7451</span><span class=p>,</span>
            <span class=mf>0.0078</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0431</span><span class=p>,</span>  <span class=mf>0.7451</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>
            <span class=mf>0.2745</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.1373</span><span class=p>,</span>  <span class=mf>0.9451</span><span class=p>,</span>
            <span class=mf>0.8824</span><span class=p>,</span>  <span class=mf>0.6275</span><span class=p>,</span>  <span class=mf>0.4235</span><span class=p>,</span>  <span class=mf>0.0039</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.3176</span><span class=p>,</span>
            <span class=mf>0.9412</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.4667</span><span class=p>,</span>  <span class=mf>0.0980</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.1765</span><span class=p>,</span>  <span class=mf>0.7294</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.5882</span><span class=p>,</span>  <span class=mf>0.1059</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0627</span><span class=p>,</span>  <span class=mf>0.3647</span><span class=p>,</span>  <span class=mf>0.9882</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.7333</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.9765</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9765</span><span class=p>,</span>  <span class=mf>0.2510</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.1804</span><span class=p>,</span>  <span class=mf>0.5098</span><span class=p>,</span>  <span class=mf>0.7176</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.8118</span><span class=p>,</span>  <span class=mf>0.0078</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.1529</span><span class=p>,</span>  <span class=mf>0.5804</span><span class=p>,</span>
            <span class=mf>0.8980</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9804</span><span class=p>,</span>  <span class=mf>0.7137</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0941</span><span class=p>,</span>  <span class=mf>0.4471</span><span class=p>,</span>  <span class=mf>0.8667</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>
            <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.7882</span><span class=p>,</span>  <span class=mf>0.3059</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0902</span><span class=p>,</span>  <span class=mf>0.2588</span><span class=p>,</span>  <span class=mf>0.8353</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>
            <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.7765</span><span class=p>,</span>  <span class=mf>0.3176</span><span class=p>,</span>  <span class=mf>0.0078</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0706</span><span class=p>,</span>
            <span class=mf>0.6706</span><span class=p>,</span>  <span class=mf>0.8588</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.7647</span><span class=p>,</span>
            <span class=mf>0.3137</span><span class=p>,</span>  <span class=mf>0.0353</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.2157</span><span class=p>,</span>  <span class=mf>0.6745</span><span class=p>,</span>  <span class=mf>0.8863</span><span class=p>,</span>
            <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9569</span><span class=p>,</span>  <span class=mf>0.5216</span><span class=p>,</span>  <span class=mf>0.0431</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.5333</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.9922</span><span class=p>,</span>
            <span class=mf>0.9922</span><span class=p>,</span>  <span class=mf>0.8314</span><span class=p>,</span>  <span class=mf>0.5294</span><span class=p>,</span>  <span class=mf>0.5176</span><span class=p>,</span>  <span class=mf>0.0627</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>],</span>
          <span class=p>[</span> <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
            <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>]]]),</span>
 <span class=n>tensor</span><span class=p>(</span><span class=mi>5</span><span class=p>))</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Inspecting a single data point in the training dataset</p> <p>When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label.</p> <div class=highlight><pre><span></span><code><span class=nb>type</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=nb>tuple</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Inspecting training dataset first element of tuple</p> <p>This means to access the image, you need to access the first element in the tuple.</p> <div class=highlight><pre><span></span><code><span class=c1># Input Matrix</span>
<span class=n>train_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=c1># A 28x28 sized image of a digit</span>
<span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>])</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Inspecting training dataset second element of tuple</p> <p>The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5.</p> <div class=highlight><pre><span></span><code><span class=c1># Label</span>
<span class=n>train_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>tensor</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
</code></pre></div> <h4 id=displaying-mnist>Displaying MNIST<a class=headerlink href=#displaying-mnist title="Permanent link">&para;</a></h4> <div class="admonition note"> <p class=admonition-title>Verifying shape of MNIST image</p> <p>As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=o>%</span><span class=n>matplotlib</span> <span class=n>inline</span>  
<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>train_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Plot image of MNIST image</p> <div class=highlight><pre><span></span><code><span class=n>show_img</span> <span class=o>=</span> <span class=n>train_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>show_img</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
</code></pre></div> </div> <p><img alt=png src=../pytorch_logistic_regression_files/pytorch_logistic_regression_24_1.png></p> <div class="admonition note"> <p class=admonition-title>Second element of tuple shows label</p> <p>As you would expect, the label is 5. <div class=highlight><pre><span></span><code><span class=c1># Label</span>
<span class=n>train_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
</code></pre></div></p> </div> <div class=highlight><pre><span></span><code><span class=n>tensor</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Plot second image of MNIST image</p> <div class=highlight><pre><span></span><code><span class=n>show_img</span> <span class=o>=</span> <span class=n>train_dataset</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>show_img</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
</code></pre></div> </div> <p><img alt=png src=../pytorch_logistic_regression_files/pytorch_logistic_regression_27_1.png></p> <div class="admonition note"> <p class=admonition-title>Second element of tuple shows label</p> <p>We should see 0 here as the label. <div class=highlight><pre><span></span><code><span class=c1># Label</span>
<span class=n>train_dataset</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
</code></pre></div></p> </div> <div class=highlight><pre><span></span><code><span class=n>tensor</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</code></pre></div> <h3 id=step-1b-loading-mnist-test-dataset>Step 1b: Loading MNIST Test Dataset<a class=headerlink href=#step-1b-loading-mnist-test-dataset title="Permanent link">&para;</a></h3> <ul> <li>Show our algorithm works beyond the data we have trained on.</li> <li>Out-of-sample</li> </ul> <div class="admonition note"> <p class=admonition-title>Load test dataset</p> <p>Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance. <div class=highlight><pre><span></span><code><span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>
</code></pre></div></p> <div class=highlight><pre><span></span><code><span class=nb>len</span><span class=p>(</span><span class=n>test_dataset</span><span class=p>)</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=mi>10000</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Test dataset elements</p> <p>Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels. <div class=highlight><pre><span></span><code><span class=nb>type</span><span class=p>(</span><span class=n>test_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</code></pre></div></p> </div> <div class=highlight><pre><span></span><code><span class=nb>tuple</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Test dataset first element in tuple</p> <p>This contains the image matrix, similar to the training set. <div class=highlight><pre><span></span><code><span class=c1># Image matrix</span>
<span class=n>test_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</code></pre></div></p> </div> <div class=highlight><pre><span></span><code><span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>])</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Plot image sample from test dataset</p> <div class=highlight><pre><span></span><code><span class=n>show_img</span> <span class=o>=</span> <span class=n>test_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>show_img</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
</code></pre></div> </div> <p><img alt=png src=../pytorch_logistic_regression_files/pytorch_logistic_regression_34_1.png></p> <div class="admonition note"> <p class=admonition-title>Test dataset second element in tuple</p> <div class=highlight><pre><span></span><code><span class=c1># Label</span>
<span class=n>test_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>tensor</span><span class=p>(</span><span class=mi>7</span><span class=p>)</span>
</code></pre></div> <h3 id=step-2-make-dataset-iterable>Step 2: Make Dataset Iterable<a class=headerlink href=#step-2-make-dataset-iterable title="Permanent link">&para;</a></h3> <ul> <li>Aim: make the dataset iterable</li> <li><strong>totaldata</strong>: 60000</li> <li><strong>minibatch</strong>: 100<ul> <li>Number of examples in 1 iteration</li> </ul> </li> <li><strong>iterations</strong>: 3000<ul> <li>1 iteration: one mini-batch forward &amp; backward pass</li> </ul> </li> <li><strong>epochs</strong><ul> <li>1 epoch: running through the whole dataset once</li> <li><span><span class=MathJax_Preview>epochs = iterations \div \frac{totaldata}{minibatch} = 3000 \div \frac{60000}{100} = 5</span><script type=math/tex>epochs = iterations \div \frac{totaldata}{minibatch} = 3000 \div \frac{60000}{100} = 5</script></span></li> </ul> </li> </ul> <div class="admonition note"> <p class=admonition-title>Recap training dataset</p> <p>Remember training dataset has 60k images and testing dataset has 10k images. <div class=highlight><pre><span></span><code><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span>
</code></pre></div></p> </div> <div class=highlight><pre><span></span><code><span class=mi>60000</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Defining epochs</p> <p>When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch. </p> <p>However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration.</p> <div class=highlight><pre><span></span><code><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
</code></pre></div> <p>We arbitrarily set 3000 iterations here which means the model would update 3000 times. <div class=highlight><pre><span></span><code><span class=n>n_iters</span> <span class=o>=</span> <span class=mi>3000</span>
</code></pre></div></p> <p>One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations. </p> <div class=highlight><pre><span></span><code><span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>
<span class=n>num_epochs</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=mi>5</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Create Iterable Object: Training Dataset</p> <div class=highlight><pre><span></span><code><span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> </div> <div class="admonition note"> <p class=admonition-title>Check Iterability</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>collections</span>
<span class=nb>isinstance</span><span class=p>(</span><span class=n>train_loader</span><span class=p>,</span> <span class=n>collections</span><span class=o>.</span><span class=n>Iterable</span><span class=p>)</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=kc>True</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Create Iterable Object: Testing Dataset</p> <div class=highlight><pre><span></span><code><span class=c1># Iterable object</span>
<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</code></pre></div> </div> <div class="admonition note"> <p class=admonition-title>Check iterability of testing dataset</p> <div class=highlight><pre><span></span><code><span class=nb>isinstance</span><span class=p>(</span><span class=n>test_loader</span><span class=p>,</span> <span class=n>collections</span><span class=o>.</span><span class=n>Iterable</span><span class=p>)</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=kc>True</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Iterate through dataset</p> <p>This is just a simplified example of what we're doing above where we're creating an iterable object <code>lst</code> to loop through so we can access all the images <code>img_1</code> and <code>img_2</code>.</p> <p>Above, the equivalent of <code>lst</code> is <code>train_loader</code> and <code>test_loader</code>.</p> <div class=highlight><pre><span></span><code><span class=n>img_1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>))</span>
<span class=n>img_2</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>))</span>
<span class=n>lst</span> <span class=o>=</span> <span class=p>[</span><span class=n>img_1</span><span class=p>,</span> <span class=n>img_2</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Need to iterate</span>
<span class=c1># Think of numbers as the images</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>lst</span><span class=p>:</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>i</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)</span>
<span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)</span>
</code></pre></div> <h3 id=step-3-building-model>Step 3: Building Model<a class=headerlink href=#step-3-building-model title="Permanent link">&para;</a></h3> <div class="admonition note"> <p class=admonition-title>Create model class</p> <div class=highlight><pre><span></span><code><span class=c1># Same as linear regression! </span>
<span class=k>class</span> <span class=nc>LogisticRegressionModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>LogisticRegressionModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>
</code></pre></div> </div> <h3 id=step-4-instantiate-model-class>Step 4: Instantiate Model Class<a class=headerlink href=#step-4-instantiate-model-class title="Permanent link">&para;</a></h3> <ul> <li>Input dimension: <ul> <li>Size of image</li> <li><span><span class=MathJax_Preview>28 \times 28 = 784</span><script type=math/tex>28 \times 28 = 784</script></span></li> </ul> </li> <li>Output dimension: 10<ul> <li>0, 1, 2, 3, 4, 5, 6, 7, 8, 9</li> </ul> </li> </ul> <div class="admonition note"> <p class=admonition-title>Check size of dataset</p> <p>This should be 28x28. <div class=highlight><pre><span></span><code><span class=c1># Size of images</span>
<span class=n>train_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</code></pre></div></p> </div> <div class=highlight><pre><span></span><code><span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>])</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Instantiate model class based on input and out dimensions</p> <p>As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10. </p> <p>And we're feeding the model with 28x28 images, hence our input dimension is 28x28. <div class=highlight><pre><span></span><code><span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>LogisticRegressionModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>
</code></pre></div></p> </div> <h3 id=step-5-instantiate-loss-class>Step 5: Instantiate Loss Class<a class=headerlink href=#step-5-instantiate-loss-class title="Permanent link">&para;</a></h3> <ul> <li><strong>Logistic Regression</strong>: Cross Entropy Loss<ul> <li><em>Linear Regression: MSE</em></li> </ul> </li> </ul> <div class="admonition note"> <p class=admonition-title>Create Cross Entry Loss Class</p> <p>Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters.</p> <div class=highlight><pre><span></span><code><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>  
</code></pre></div> </div> <div class="admonition alert"> <p class=admonition-title>What happens in nn.CrossEntropyLoss()?</p> <p>It does 2 things at the same time. </p> <p><br> 1. Computes softmax (logistic/softmax function) <br> 2. Computes cross entropy</p> </div> <p><img alt src=../images/cross_entropy_final_4.png></p> <h3 id=step-6-instantiate-optimizer-class>Step 6: Instantiate Optimizer Class<a class=headerlink href=#step-6-instantiate-optimizer-class title="Permanent link">&para;</a></h3> <ul> <li>Simplified equation<ul> <li><span><span class=MathJax_Preview>\theta = \theta - \eta \cdot \nabla_\theta</span><script type=math/tex>\theta = \theta - \eta \cdot \nabla_\theta</script></span><ul> <li><span><span class=MathJax_Preview>\theta</span><script type=math/tex>\theta</script></span>: parameters (our variables)</li> <li><span><span class=MathJax_Preview>\eta</span><script type=math/tex>\eta</script></span>: learning rate (how fast we want to learn)</li> <li><span><span class=MathJax_Preview>\nabla_\theta</span><script type=math/tex>\nabla_\theta</script></span>: parameters' gradients</li> </ul> </li> </ul> </li> <li>Even simplier equation<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> <li><strong>At every iteration, we update our model's parameters</strong></li> </ul> </li> </ul> <div class="admonition note"> <p class=admonition-title>Create optimizer</p> <p>Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently. <div class=highlight><pre><span></span><code><span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.001</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>)</span>  
</code></pre></div></p> </div> <div class="admonition note"> <p class=admonition-title>Parameters In-Depth</p> <p>You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the <span><span class=MathJax_Preview>y = AX + b</span><script type=math/tex>y = AX + b</script></span> equation where X is our input of size 784.</p> <p>We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. </p> <div class=highlight><pre><span></span><code><span class=c1># Type of parameter object</span>
<span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>

<span class=c1># Length of parameters</span>
<span class=nb>print</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())))</span>

<span class=c1># FC 1 Parameters </span>
<span class=nb>print</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>

<span class=c1># FC 1 Bias Parameters</span>
<span class=nb>print</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=o>&lt;</span><span class=n>generator</span> <span class=nb>object</span> <span class=n>Module</span><span class=o>.</span><span class=n>parameters</span> <span class=n>at</span> <span class=mh>0x7ff7c884f830</span><span class=o>&gt;</span>
<span class=mi>2</span>
<span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>10</span><span class=p>,</span> <span class=mi>784</span><span class=p>])</span>
<span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>10</span><span class=p>])</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Quick Matrix Product Review</p> <ul> <li>Example 1: <strong>matrix product</strong><ul> <li><span><span class=MathJax_Preview>A: (100, 10)</span><script type=math/tex>A: (100, 10)</script></span></li> <li><span><span class=MathJax_Preview>B: (10, 1)</span><script type=math/tex>B: (10, 1)</script></span></li> <li><span><span class=MathJax_Preview>A \cdot B = (100, 10) \cdot (10, 1) = (100, 1)</span><script type=math/tex>A \cdot B = (100, 10) \cdot (10, 1) = (100, 1)</script></span></li> </ul> </li> <li>Example 2: <strong>matrix product</strong><ul> <li><span><span class=MathJax_Preview>A: (50, 5)</span><script type=math/tex>A: (50, 5)</script></span></li> <li><span><span class=MathJax_Preview>B: (5, 2)</span><script type=math/tex>B: (5, 2)</script></span></li> <li><span><span class=MathJax_Preview>A \cdot B = (50, 5) \cdot (5, 2) = (50, 2)</span><script type=math/tex>A \cdot B = (50, 5) \cdot (5, 2) = (50, 2)</script></span></li> </ul> </li> <li>Example 3: <strong>element-wise addition</strong><ul> <li><span><span class=MathJax_Preview>A: (10, 1)</span><script type=math/tex>A: (10, 1)</script></span></li> <li><span><span class=MathJax_Preview>B: (10, 1)</span><script type=math/tex>B: (10, 1)</script></span></li> <li><span><span class=MathJax_Preview>A + B = (10, 1)</span><script type=math/tex>A + B = (10, 1)</script></span></li> </ul> </li> </ul> </div> <p><img alt src=../images/lr_params2.png></p> <h3 id=step-7-train-model>Step 7: Train Model<a class=headerlink href=#step-7-train-model title="Permanent link">&para;</a></h3> <div class="admonition note"> <p class=admonition-title>7 step process for training models</p> <ul> <li>Process <ol> <li>Convert inputs/labels to tensors with gradients</li> <li>Clear gradient buffets</li> <li>Get output given inputs</li> <li>Get loss</li> <li>Get gradients w.r.t. parameters</li> <li>Update parameters using gradients<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> </ul> </li> <li>REPEAT</li> </ol> </li> </ul> <div class=highlight><pre><span></span><code><span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images as Variable</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>
        <span class=n>labels</span> <span class=o>=</span> <span class=n>labels</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1># Total correct predictions</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Iteration: </span><span class=si>{}</span><span class=s1>. Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>Iteration</span><span class=p>:</span> <span class=mf>500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.8513233661651611</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>70</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.5732524394989014</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>77</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.3840199708938599</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>79</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.1711134910583496</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>81</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.1094708442687988</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>82</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>3000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.002761721611023</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>82</span>
</code></pre></div> <h4 id=break-down-accuracy-calculation>Break Down Accuracy Calculation<a class=headerlink href=#break-down-accuracy-calculation title="Permanent link">&para;</a></h4> <div class="admonition note"> <p class=admonition-title>Printing outputs of our model</p> <p>As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model.</p> <p>This would print out the output of the model's predictions on your notebook.</p> <div class=highlight><pre><span></span><code><span class=n>iter_test</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
    <span class=n>iter_test</span> <span class=o>+=</span> <span class=mi>1</span>
    <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>
    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>iter_test</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;OUTPUTS&#39;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=p>)</span>
    <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>OUTPUTS</span>
<span class=n>tensor</span><span class=p>([[</span><span class=o>-</span><span class=mf>0.4181</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0784</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4840</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0985</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2394</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1801</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1639</span><span class=p>,</span>
          <span class=mf>2.9352</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1552</span><span class=p>,</span>  <span class=mf>0.8852</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.5117</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1099</span><span class=p>,</span>  <span class=mf>1.5295</span><span class=p>,</span>  <span class=mf>0.8863</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.8813</span><span class=p>,</span>  <span class=mf>0.5967</span><span class=p>,</span>  <span class=mf>1.3632</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>1.8977</span><span class=p>,</span>  <span class=mf>0.4183</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.4990</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.0126</span><span class=p>,</span>  <span class=mf>2.4112</span><span class=p>,</span>  <span class=mf>0.2373</span><span class=p>,</span>  <span class=mf>0.0857</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7007</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2015</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3428</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.2548</span><span class=p>,</span>  <span class=mf>0.1659</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4703</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>2.8072</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.2973</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0984</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4313</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9619</span><span class=p>,</span>  <span class=mf>0.8670</span><span class=p>,</span>  <span class=mf>1.2201</span><span class=p>,</span>
          <span class=mf>0.3752</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2873</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3272</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.0343</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.0043</span><span class=p>,</span>  <span class=mf>0.5081</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6452</span><span class=p>,</span>  <span class=mf>1.8647</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6924</span><span class=p>,</span>  <span class=mf>0.1435</span><span class=p>,</span>
          <span class=mf>0.4330</span><span class=p>,</span>  <span class=mf>0.2958</span><span class=p>,</span>  <span class=mf>1.0339</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.5392</span><span class=p>,</span>  <span class=mf>2.9070</span><span class=p>,</span>  <span class=mf>0.2297</span><span class=p>,</span>  <span class=mf>0.3139</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6863</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2734</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8377</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.1238</span><span class=p>,</span>  <span class=mf>0.3285</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3004</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.2037</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.3739</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5947</span><span class=p>,</span>  <span class=mf>0.3530</span><span class=p>,</span>  <span class=mf>1.4205</span><span class=p>,</span>  <span class=mf>0.0593</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7307</span><span class=p>,</span>
          <span class=mf>0.6642</span><span class=p>,</span>  <span class=mf>0.3937</span><span class=p>,</span>  <span class=mf>0.8004</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.4439</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3284</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7652</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0952</span><span class=p>,</span>  <span class=mf>0.9323</span><span class=p>,</span>  <span class=mf>0.3006</span><span class=p>,</span>  <span class=mf>0.0238</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.0810</span><span class=p>,</span>  <span class=mf>0.0612</span><span class=p>,</span>  <span class=mf>1.3295</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.5409</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5266</span><span class=p>,</span>  <span class=mf>0.9914</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2369</span><span class=p>,</span>  <span class=mf>0.6583</span><span class=p>,</span>  <span class=mf>0.0992</span><span class=p>,</span>  <span class=mf>0.8525</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>1.0562</span><span class=p>,</span>  <span class=mf>0.2013</span><span class=p>,</span>  <span class=mf>0.0462</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.6548</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7253</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9825</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1663</span><span class=p>,</span>  <span class=mf>0.9076</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0694</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3708</span><span class=p>,</span>
          <span class=mf>1.8270</span><span class=p>,</span>  <span class=mf>0.2457</span><span class=p>,</span>  <span class=mf>1.5921</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>3.2147</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.7689</span><span class=p>,</span>  <span class=mf>0.8531</span><span class=p>,</span>  <span class=mf>1.2320</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8126</span><span class=p>,</span>  <span class=mf>1.1251</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2776</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>1.4244</span><span class=p>,</span>  <span class=mf>0.5930</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.6183</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.7470</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5545</span><span class=p>,</span>  <span class=mf>1.0251</span><span class=p>,</span>  <span class=mf>0.0529</span><span class=p>,</span>  <span class=mf>0.4384</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5934</span><span class=p>,</span>  <span class=mf>0.7666</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>1.0084</span><span class=p>,</span>  <span class=mf>0.5313</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3465</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.7916</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.7064</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7805</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1588</span><span class=p>,</span>  <span class=mf>1.3284</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1708</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2092</span><span class=p>,</span>
          <span class=mf>0.9495</span><span class=p>,</span>  <span class=mf>0.1033</span><span class=p>,</span>  <span class=mf>2.0208</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>3.0602</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.3578</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2576</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2198</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2372</span><span class=p>,</span>  <span class=mf>0.9765</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1514</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.5380</span><span class=p>,</span>  <span class=mf>0.7970</span><span class=p>,</span>  <span class=mf>0.1374</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.2613</span><span class=p>,</span>  <span class=mf>2.8594</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0874</span><span class=p>,</span>  <span class=mf>0.1974</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2018</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0064</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0923</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.2142</span><span class=p>,</span>  <span class=mf>0.2575</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3218</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.4348</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7216</span><span class=p>,</span>  <span class=mf>0.0021</span><span class=p>,</span>  <span class=mf>1.2864</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5062</span><span class=p>,</span>  <span class=mf>0.7761</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3236</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.5667</span><span class=p>,</span>  <span class=mf>0.5431</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7781</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.2157</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.0200</span><span class=p>,</span>  <span class=mf>0.1829</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6882</span><span class=p>,</span>  <span class=mf>1.3815</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7609</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0902</span><span class=p>,</span>
          <span class=mf>0.8647</span><span class=p>,</span>  <span class=mf>0.3679</span><span class=p>,</span>  <span class=mf>1.8843</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.0950</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.5009</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6347</span><span class=p>,</span>  <span class=mf>0.3662</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4679</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0359</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7671</span><span class=p>,</span>
          <span class=mf>2.7155</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3991</span><span class=p>,</span>  <span class=mf>0.5737</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.7005</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5366</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0434</span><span class=p>,</span>  <span class=mf>1.1289</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5873</span><span class=p>,</span>  <span class=mf>0.2555</span><span class=p>,</span>  <span class=mf>0.8187</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.6557</span><span class=p>,</span>  <span class=mf>0.1241</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4297</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.0635</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.5991</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4677</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1231</span><span class=p>,</span>  <span class=mf>2.0445</span><span class=p>,</span>  <span class=mf>0.1128</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1825</span><span class=p>,</span>
          <span class=mf>0.1075</span><span class=p>,</span>  <span class=mf>0.0348</span><span class=p>,</span>  <span class=mf>1.4317</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.0319</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1595</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.3415</span><span class=p>,</span>  <span class=mf>0.1095</span><span class=p>,</span>  <span class=mf>0.5339</span><span class=p>,</span>  <span class=mf>0.1973</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.3272</span><span class=p>,</span>
          <span class=mf>1.5765</span><span class=p>,</span>  <span class=mf>0.4784</span><span class=p>,</span>  <span class=mf>1.4176</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.4928</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.5653</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0672</span><span class=p>,</span>  <span class=mf>0.3325</span><span class=p>,</span>  <span class=mf>0.5359</span><span class=p>,</span>  <span class=mf>0.5368</span><span class=p>,</span>  <span class=mf>2.1542</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>1.4276</span><span class=p>,</span>  <span class=mf>0.3605</span><span class=p>,</span>  <span class=mf>0.0587</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.4761</span><span class=p>,</span>  <span class=mf>0.2958</span><span class=p>,</span>  <span class=mf>0.6597</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2658</span><span class=p>,</span>  <span class=mf>1.1279</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0676</span><span class=p>,</span>  <span class=mf>1.2506</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.2059</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1489</span><span class=p>,</span>  <span class=mf>0.1051</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.0764</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9274</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6838</span><span class=p>,</span>  <span class=mf>0.3464</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2656</span><span class=p>,</span>  <span class=mf>1.4099</span><span class=p>,</span>  <span class=mf>0.4486</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.9527</span><span class=p>,</span>  <span class=mf>0.5682</span><span class=p>,</span>  <span class=mf>0.0156</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.6900</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9611</span><span class=p>,</span>  <span class=mf>0.1395</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0079</span><span class=p>,</span>  <span class=mf>1.5424</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3208</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2682</span><span class=p>,</span>
          <span class=mf>0.3586</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2771</span><span class=p>,</span>  <span class=mf>1.0389</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>4.3606</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.8621</span><span class=p>,</span>  <span class=mf>0.6310</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9657</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2486</span><span class=p>,</span>  <span class=mf>1.2009</span><span class=p>,</span>  <span class=mf>1.1873</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.8255</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2103</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2172</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.1000</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.4268</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4627</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1041</span><span class=p>,</span>  <span class=mf>0.2959</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1392</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6855</span><span class=p>,</span>
          <span class=mf>1.8622</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2580</span><span class=p>,</span>  <span class=mf>1.1347</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.3625</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.1323</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2224</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8754</span><span class=p>,</span>  <span class=mf>2.4684</span><span class=p>,</span>  <span class=mf>0.0295</span><span class=p>,</span>  <span class=mf>0.1161</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.2660</span><span class=p>,</span>  <span class=mf>0.3037</span><span class=p>,</span>  <span class=mf>1.4570</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>2.8688</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.4517</span><span class=p>,</span>  <span class=mf>0.1782</span><span class=p>,</span>  <span class=mf>1.1149</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0898</span><span class=p>,</span>  <span class=mf>1.1062</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0681</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.5697</span><span class=p>,</span>  <span class=mf>0.8888</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6965</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.0429</span><span class=p>,</span>  <span class=mf>1.4446</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3349</span><span class=p>,</span>  <span class=mf>0.1254</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5017</span><span class=p>,</span>  <span class=mf>0.2286</span><span class=p>,</span>  <span class=mf>0.2328</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.3290</span><span class=p>,</span>  <span class=mf>0.3949</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2586</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.8476</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0004</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1003</span><span class=p>,</span>  <span class=mf>2.2806</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2226</span><span class=p>,</span>  <span class=mf>0.9251</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3165</span><span class=p>,</span>
          <span class=mf>0.4957</span><span class=p>,</span>  <span class=mf>0.0690</span><span class=p>,</span>  <span class=mf>0.0232</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.9108</span><span class=p>,</span>  <span class=mf>1.1355</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2715</span><span class=p>,</span>  <span class=mf>0.2233</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3681</span><span class=p>,</span>  <span class=mf>0.1442</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0001</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.0174</span><span class=p>,</span>  <span class=mf>0.1454</span><span class=p>,</span>  <span class=mf>0.2286</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.0663</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8466</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7147</span><span class=p>,</span>  <span class=mf>2.5685</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2090</span><span class=p>,</span>  <span class=mf>1.2993</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3057</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.8314</span><span class=p>,</span>  <span class=mf>0.7046</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0176</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>1.7013</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.8051</span><span class=p>,</span>  <span class=mf>0.7541</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.5248</span><span class=p>,</span>  <span class=mf>0.8972</span><span class=p>,</span>  <span class=mf>0.1518</span><span class=p>,</span>  <span class=mf>1.4876</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.8454</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2022</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2829</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.8179</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1239</span><span class=p>,</span>  <span class=mf>0.8630</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2137</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2275</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5411</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.3448</span><span class=p>,</span>
          <span class=mf>1.7354</span><span class=p>,</span>  <span class=mf>0.7751</span><span class=p>,</span>  <span class=mf>0.6234</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.6515</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0431</span><span class=p>,</span>  <span class=mf>2.7165</span><span class=p>,</span>  <span class=mf>0.1873</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0623</span><span class=p>,</span>  <span class=mf>0.1286</span><span class=p>,</span>  <span class=mf>0.3597</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.2739</span><span class=p>,</span>  <span class=mf>0.3871</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.6699</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.2828</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.4663</span><span class=p>,</span>  <span class=mf>0.1182</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0896</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3640</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5129</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4905</span><span class=p>,</span>
          <span class=mf>2.2914</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2227</span><span class=p>,</span>  <span class=mf>0.9463</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.2596</span><span class=p>,</span>  <span class=mf>2.0468</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4405</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0411</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8073</span><span class=p>,</span>  <span class=mf>0.0490</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0604</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.1206</span><span class=p>,</span>  <span class=mf>0.3504</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1059</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.6089</span><span class=p>,</span>  <span class=mf>0.5885</span><span class=p>,</span>  <span class=mf>0.7898</span><span class=p>,</span>  <span class=mf>1.1318</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.9008</span><span class=p>,</span>  <span class=mf>0.5875</span><span class=p>,</span>  <span class=mf>0.4227</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>1.1815</span><span class=p>,</span>  <span class=mf>0.5652</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.3590</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.4551</span><span class=p>,</span>  <span class=mf>2.9537</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2805</span><span class=p>,</span>  <span class=mf>0.2372</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.4180</span><span class=p>,</span>  <span class=mf>0.0297</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1515</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.6111</span><span class=p>,</span>  <span class=mf>0.6140</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3354</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.7182</span><span class=p>,</span>  <span class=mf>1.6778</span><span class=p>,</span>  <span class=mf>0.0553</span><span class=p>,</span>  <span class=mf>0.0461</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5446</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0338</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0215</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.0881</span><span class=p>,</span>  <span class=mf>0.1506</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2107</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.8027</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7854</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1275</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3177</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1600</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1964</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6084</span><span class=p>,</span>
          <span class=mf>2.1285</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1815</span><span class=p>,</span>  <span class=mf>1.1911</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>2.0656</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4959</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1154</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1363</span><span class=p>,</span>  <span class=mf>2.2426</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7441</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8413</span><span class=p>,</span>
          <span class=mf>0.4675</span><span class=p>,</span>  <span class=mf>0.3269</span><span class=p>,</span>  <span class=mf>1.7279</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.3004</span><span class=p>,</span>  <span class=mf>1.0166</span><span class=p>,</span>  <span class=mf>1.1175</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0618</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0937</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4221</span><span class=p>,</span>  <span class=mf>0.1943</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>1.1020</span><span class=p>,</span>  <span class=mf>0.3670</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4683</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.0720</span><span class=p>,</span>  <span class=mf>0.2252</span><span class=p>,</span>  <span class=mf>0.0175</span><span class=p>,</span>  <span class=mf>1.3644</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7409</span><span class=p>,</span>  <span class=mf>0.4655</span><span class=p>,</span>  <span class=mf>0.5439</span><span class=p>,</span>
          <span class=mf>0.0380</span><span class=p>,</span>  <span class=mf>0.1279</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2302</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.2409</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2622</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6336</span><span class=p>,</span>  <span class=mf>1.8240</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5951</span><span class=p>,</span>  <span class=mf>1.3408</span><span class=p>,</span>  <span class=mf>0.2130</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>1.3789</span><span class=p>,</span>  <span class=mf>0.8363</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2101</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.3849</span><span class=p>,</span>  <span class=mf>0.3773</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0585</span><span class=p>,</span>  <span class=mf>0.6896</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0998</span><span class=p>,</span>  <span class=mf>0.2804</span><span class=p>,</span>  <span class=mf>0.0696</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.2529</span><span class=p>,</span>  <span class=mf>0.3143</span><span class=p>,</span>  <span class=mf>0.3409</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.9103</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1578</span><span class=p>,</span>  <span class=mf>1.6673</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4817</span><span class=p>,</span>  <span class=mf>0.4088</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5484</span><span class=p>,</span>  <span class=mf>0.6103</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.2287</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0665</span><span class=p>,</span>  <span class=mf>0.0055</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.1692</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.8531</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2499</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0257</span><span class=p>,</span>  <span class=mf>2.8580</span><span class=p>,</span>  <span class=mf>0.2616</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7122</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.0551</span><span class=p>,</span>  <span class=mf>0.8112</span><span class=p>,</span>  <span class=mf>2.3233</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.2790</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.9494</span><span class=p>,</span>  <span class=mf>0.6096</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5653</span><span class=p>,</span>  <span class=mf>2.2792</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0687</span><span class=p>,</span>  <span class=mf>0.1634</span><span class=p>,</span>
          <span class=mf>0.3122</span><span class=p>,</span>  <span class=mf>0.1053</span><span class=p>,</span>  <span class=mf>1.0884</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.1267</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2297</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1315</span><span class=p>,</span>  <span class=mf>0.2428</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5436</span><span class=p>,</span>  <span class=mf>0.4123</span><span class=p>,</span>  <span class=mf>2.3060</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.9278</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1528</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4224</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.0235</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9137</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1457</span><span class=p>,</span>  <span class=mf>1.6858</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7552</span><span class=p>,</span>  <span class=mf>0.7293</span><span class=p>,</span>  <span class=mf>0.2510</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.3955</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2187</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1505</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.5643</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2783</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.4149</span><span class=p>,</span>  <span class=mf>0.0304</span><span class=p>,</span>  <span class=mf>0.8375</span><span class=p>,</span>  <span class=mf>1.5018</span><span class=p>,</span>  <span class=mf>0.0338</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.3875</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0117</span><span class=p>,</span>  <span class=mf>0.5751</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.2926</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7486</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3238</span><span class=p>,</span>  <span class=mf>1.0384</span><span class=p>,</span>  <span class=mf>0.0308</span><span class=p>,</span>  <span class=mf>0.6792</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0170</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.5797</span><span class=p>,</span>  <span class=mf>0.2819</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3510</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.1219</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5862</span><span class=p>,</span>  <span class=mf>1.5817</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1297</span><span class=p>,</span>  <span class=mf>0.4730</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9171</span><span class=p>,</span>  <span class=mf>0.7886</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.7022</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0501</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2812</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>1.7587</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.4511</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7369</span><span class=p>,</span>  <span class=mf>0.4082</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6426</span><span class=p>,</span>  <span class=mf>1.1784</span><span class=p>,</span>  <span class=mf>0.6052</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.7178</span><span class=p>,</span>  <span class=mf>1.6161</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2220</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.1267</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.6719</span><span class=p>,</span>  <span class=mf>0.0505</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4972</span><span class=p>,</span>  <span class=mf>2.9027</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1461</span><span class=p>,</span>  <span class=mf>0.2807</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.2921</span><span class=p>,</span>  <span class=mf>0.2231</span><span class=p>,</span>  <span class=mf>1.1327</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.9892</span><span class=p>,</span>  <span class=mf>2.4401</span><span class=p>,</span>  <span class=mf>0.1274</span><span class=p>,</span>  <span class=mf>0.2838</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7535</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1684</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6493</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.1908</span><span class=p>,</span>  <span class=mf>0.2290</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2150</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.2071</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.1351</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9191</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9309</span><span class=p>,</span>  <span class=mf>1.7747</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3046</span><span class=p>,</span>  <span class=mf>0.0183</span><span class=p>,</span>
          <span class=mf>1.0136</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1016</span><span class=p>,</span>  <span class=mf>2.1288</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.0103</span><span class=p>,</span>  <span class=mf>0.3280</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6974</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2504</span><span class=p>,</span>  <span class=mf>0.3187</span><span class=p>,</span>  <span class=mf>0.4390</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1879</span><span class=p>,</span>
          <span class=mf>0.3954</span><span class=p>,</span>  <span class=mf>0.2332</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1971</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.2280</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.6754</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7438</span><span class=p>,</span>  <span class=mf>0.5078</span><span class=p>,</span>  <span class=mf>0.2544</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1020</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2503</span><span class=p>,</span>
          <span class=mf>2.0799</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5033</span><span class=p>,</span>  <span class=mf>0.5890</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.3972</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9369</span><span class=p>,</span>  <span class=mf>1.2696</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.6713</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4159</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0221</span><span class=p>,</span>  <span class=mf>0.6489</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.4777</span><span class=p>,</span>  <span class=mf>1.2497</span><span class=p>,</span>  <span class=mf>0.3931</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.7566</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8230</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0785</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3083</span><span class=p>,</span>  <span class=mf>0.7821</span><span class=p>,</span>  <span class=mf>0.1880</span><span class=p>,</span>  <span class=mf>0.1037</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.0956</span><span class=p>,</span>  <span class=mf>0.4219</span><span class=p>,</span>  <span class=mf>1.0798</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.0328</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1700</span><span class=p>,</span>  <span class=mf>1.3806</span><span class=p>,</span>  <span class=mf>0.5445</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2624</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0780</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3595</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.6253</span><span class=p>,</span>  <span class=mf>0.4309</span><span class=p>,</span>  <span class=mf>0.1813</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.0360</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4704</span><span class=p>,</span>  <span class=mf>0.1948</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7066</span><span class=p>,</span>  <span class=mf>0.6600</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4633</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3602</span><span class=p>,</span>
          <span class=mf>1.7494</span><span class=p>,</span>  <span class=mf>0.1522</span><span class=p>,</span>  <span class=mf>0.6086</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.2032</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7903</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5754</span><span class=p>,</span>  <span class=mf>0.4722</span><span class=p>,</span>  <span class=mf>0.6068</span><span class=p>,</span>  <span class=mf>0.5752</span><span class=p>,</span>  <span class=mf>0.2151</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.2495</span><span class=p>,</span>  <span class=mf>0.3420</span><span class=p>,</span>  <span class=mf>0.9278</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.2247</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1361</span><span class=p>,</span>  <span class=mf>0.9374</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1543</span><span class=p>,</span>  <span class=mf>0.4921</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6553</span><span class=p>,</span>  <span class=mf>0.5885</span><span class=p>,</span>
          <span class=mf>0.2617</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2216</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3736</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.2867</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.4486</span><span class=p>,</span>  <span class=mf>0.6658</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8755</span><span class=p>,</span>  <span class=mf>2.3195</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7627</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2132</span><span class=p>,</span>
          <span class=mf>0.2488</span><span class=p>,</span>  <span class=mf>0.3484</span><span class=p>,</span>  <span class=mf>1.0860</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.4031</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4518</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3181</span><span class=p>,</span>  <span class=mf>2.8268</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5371</span><span class=p>,</span>  <span class=mf>1.0154</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9247</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.7385</span><span class=p>,</span>  <span class=mf>1.1031</span><span class=p>,</span>  <span class=mf>0.0422</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>2.8604</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.5413</span><span class=p>,</span>  <span class=mf>0.6241</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8017</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.4104</span><span class=p>,</span>  <span class=mf>0.6314</span><span class=p>,</span>  <span class=mf>0.4614</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.0218</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3411</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2609</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.2113</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2348</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8535</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1041</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2703</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1294</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7057</span><span class=p>,</span>
          <span class=mf>2.7552</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4429</span><span class=p>,</span>  <span class=mf>0.4517</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>4.5191</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.7407</span><span class=p>,</span>  <span class=mf>1.1091</span><span class=p>,</span>  <span class=mf>0.3975</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9456</span><span class=p>,</span>  <span class=mf>1.2277</span><span class=p>,</span>  <span class=mf>0.3616</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>1.6564</span><span class=p>,</span>  <span class=mf>0.5063</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.4274</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>1.4615</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0765</span><span class=p>,</span>  <span class=mf>1.8388</span><span class=p>,</span>  <span class=mf>1.5006</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2351</span><span class=p>,</span>  <span class=mf>0.2781</span><span class=p>,</span>  <span class=mf>0.2830</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.8491</span><span class=p>,</span>  <span class=mf>0.2222</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.7779</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.2160</span><span class=p>,</span>  <span class=mf>0.8502</span><span class=p>,</span>  <span class=mf>0.2413</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0798</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7880</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4286</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8060</span><span class=p>,</span>
          <span class=mf>0.7194</span><span class=p>,</span>  <span class=mf>1.2663</span><span class=p>,</span>  <span class=mf>0.6412</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.3318</span><span class=p>,</span>  <span class=mf>2.3388</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4003</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1094</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0285</span><span class=p>,</span>  <span class=mf>0.1021</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0388</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.0497</span><span class=p>,</span>  <span class=mf>0.5137</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2507</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.7853</span><span class=p>,</span>  <span class=mf>0.5884</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6108</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5557</span><span class=p>,</span>  <span class=mf>0.8696</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6226</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7983</span><span class=p>,</span>
          <span class=mf>1.7169</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0145</span><span class=p>,</span>  <span class=mf>0.8231</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.1739</span><span class=p>,</span>  <span class=mf>0.1562</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2933</span><span class=p>,</span>  <span class=mf>2.3195</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9480</span><span class=p>,</span>  <span class=mf>1.2019</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4834</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>1.0567</span><span class=p>,</span>  <span class=mf>0.5685</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6841</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.7920</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3339</span><span class=p>,</span>  <span class=mf>0.7452</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6529</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3307</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6092</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0950</span><span class=p>,</span>
          <span class=mf>1.7311</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3481</span><span class=p>,</span>  <span class=mf>0.3801</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.7810</span><span class=p>,</span>  <span class=mf>1.0676</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7611</span><span class=p>,</span>  <span class=mf>0.3658</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0431</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1012</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6048</span><span class=p>,</span>
          <span class=mf>0.3089</span><span class=p>,</span>  <span class=mf>0.9998</span><span class=p>,</span>  <span class=mf>0.7164</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.5856</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5261</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4859</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0551</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1838</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2144</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2599</span><span class=p>,</span>
          <span class=mf>3.3891</span><span class=p>,</span>  <span class=mf>0.4691</span><span class=p>,</span>  <span class=mf>0.7566</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.4984</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.7770</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1998</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1075</span><span class=p>,</span>  <span class=mf>1.0882</span><span class=p>,</span>  <span class=mf>0.4539</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5651</span><span class=p>,</span>
          <span class=mf>1.4381</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5678</span><span class=p>,</span>  <span class=mf>1.7479</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.2938</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.8536</span><span class=p>,</span>  <span class=mf>0.4259</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5429</span><span class=p>,</span>  <span class=mf>0.0066</span><span class=p>,</span>  <span class=mf>0.4120</span><span class=p>,</span>  <span class=mf>2.3793</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.3666</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2604</span><span class=p>,</span>  <span class=mf>0.0382</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.4080</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9851</span><span class=p>,</span>  <span class=mf>4.0264</span><span class=p>,</span>  <span class=mf>0.1099</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1766</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1557</span><span class=p>,</span>  <span class=mf>0.6419</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.8147</span><span class=p>,</span>  <span class=mf>0.7535</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1452</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.4636</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.7323</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6433</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0274</span><span class=p>,</span>  <span class=mf>0.7227</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1799</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9336</span><span class=p>,</span>
          <span class=mf>2.1881</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2073</span><span class=p>,</span>  <span class=mf>1.6522</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.9617</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0348</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3980</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4738</span><span class=p>,</span>  <span class=mf>0.7790</span><span class=p>,</span>  <span class=mf>0.4671</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6115</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.7067</span><span class=p>,</span>  <span class=mf>1.3036</span><span class=p>,</span>  <span class=mf>0.4923</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.0151</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.5385</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6072</span><span class=p>,</span>  <span class=mf>0.2902</span><span class=p>,</span>  <span class=mf>3.1570</span><span class=p>,</span>  <span class=mf>0.1062</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2169</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.4491</span><span class=p>,</span>  <span class=mf>0.6326</span><span class=p>,</span>  <span class=mf>1.6829</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.8852</span><span class=p>,</span>  <span class=mf>0.6066</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2840</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4475</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1147</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7858</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1805</span><span class=p>,</span>
          <span class=mf>3.0723</span><span class=p>,</span>  <span class=mf>0.3960</span><span class=p>,</span>  <span class=mf>0.9720</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.0344</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.4878</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9675</span><span class=p>,</span>  <span class=mf>1.9649</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3146</span><span class=p>,</span>  <span class=mf>1.2183</span><span class=p>,</span>  <span class=mf>0.6730</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.3650</span><span class=p>,</span>  <span class=mf>0.0646</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0898</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.2118</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.0350</span><span class=p>,</span>  <span class=mf>0.9917</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8993</span><span class=p>,</span>  <span class=mf>1.2334</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6723</span><span class=p>,</span>  <span class=mf>2.5847</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.0454</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4149</span><span class=p>,</span>  <span class=mf>0.3927</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.7365</span><span class=p>,</span>  <span class=mf>3.0447</span><span class=p>,</span>  <span class=mf>0.5115</span><span class=p>,</span>  <span class=mf>0.0786</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7544</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2158</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4876</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.2891</span><span class=p>,</span>  <span class=mf>0.5089</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6719</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>0.3652</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5457</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1167</span><span class=p>,</span>  <span class=mf>2.9056</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1622</span><span class=p>,</span>  <span class=mf>0.8192</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.3245</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.6414</span><span class=p>,</span>  <span class=mf>0.8097</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4958</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.8755</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6983</span><span class=p>,</span>  <span class=mf>0.2208</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6463</span><span class=p>,</span>  <span class=mf>0.5276</span><span class=p>,</span>  <span class=mf>0.1145</span><span class=p>,</span>  <span class=mf>2.7229</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>1.0316</span><span class=p>,</span>  <span class=mf>0.1905</span><span class=p>,</span>  <span class=mf>0.2090</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.9702</span><span class=p>,</span>  <span class=mf>0.1265</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0007</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5106</span><span class=p>,</span>  <span class=mf>0.4970</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0804</span><span class=p>,</span>  <span class=mf>0.0017</span><span class=p>,</span>
          <span class=mf>0.0607</span><span class=p>,</span>  <span class=mf>0.6164</span><span class=p>,</span>  <span class=mf>0.4490</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.8271</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6822</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7434</span><span class=p>,</span>  <span class=mf>2.6457</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.6143</span><span class=p>,</span>  <span class=mf>1.1486</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0705</span><span class=p>,</span>
          <span class=mf>0.5611</span><span class=p>,</span>  <span class=mf>0.6422</span><span class=p>,</span>  <span class=mf>0.1250</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.9979</span><span class=p>,</span>  <span class=mf>1.8175</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1658</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0343</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6292</span><span class=p>,</span>  <span class=mf>0.1774</span><span class=p>,</span>  <span class=mf>0.3150</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.4633</span><span class=p>,</span>  <span class=mf>0.9266</span><span class=p>,</span>  <span class=mf>0.0252</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.9039</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6030</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2173</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1768</span><span class=p>,</span>  <span class=mf>2.3198</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5072</span><span class=p>,</span>  <span class=mf>0.3418</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.1551</span><span class=p>,</span>  <span class=mf>0.1282</span><span class=p>,</span>  <span class=mf>1.4250</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>0.9891</span><span class=p>,</span>  <span class=mf>0.5212</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4518</span><span class=p>,</span>  <span class=mf>0.3267</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0759</span><span class=p>,</span>  <span class=mf>0.3826</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0341</span><span class=p>,</span>
          <span class=mf>0.0382</span><span class=p>,</span>  <span class=mf>0.2451</span><span class=p>,</span>  <span class=mf>0.3658</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>2.1217</span><span class=p>,</span>  <span class=mf>1.5102</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7828</span><span class=p>,</span>  <span class=mf>0.3554</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4192</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0772</span><span class=p>,</span>  <span class=mf>0.0578</span><span class=p>,</span>
          <span class=mf>0.8070</span><span class=p>,</span>  <span class=mf>0.1701</span><span class=p>,</span>  <span class=mf>0.5880</span><span class=p>],</span>
        <span class=p>[</span> <span class=mf>1.0665</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.3826</span><span class=p>,</span>  <span class=mf>0.6243</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8096</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4227</span><span class=p>,</span>  <span class=mf>0.5925</span><span class=p>,</span>  <span class=mf>1.8112</span><span class=p>,</span>
         <span class=o>-</span><span class=mf>0.9946</span><span class=p>,</span>  <span class=mf>0.2010</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7731</span><span class=p>],</span>
        <span class=p>[</span><span class=o>-</span><span class=mf>1.1263</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.7484</span><span class=p>,</span>  <span class=mf>0.0041</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5439</span><span class=p>,</span>  <span class=mf>1.7242</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9475</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3835</span><span class=p>,</span>
          <span class=mf>0.8452</span><span class=p>,</span>  <span class=mf>0.3077</span><span class=p>,</span>  <span class=mf>2.2689</span><span class=p>]])</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Printing output size</p> <p>This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting. <div class=highlight><pre><span></span><code><span class=n>iter_test</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
    <span class=n>iter_test</span> <span class=o>+=</span> <span class=mi>1</span>
    <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>
    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>iter_test</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;OUTPUTS&#39;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
    <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</code></pre></div></p> </div> <div class=highlight><pre><span></span><code><span class=n>OUTPUTS</span>
<span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>100</span><span class=p>,</span> <span class=mi>10</span><span class=p>])</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Printing one output</p> <p>This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7.</p> <p>number 0: -0.4181 <br> number 1: -1.0784 <br>... <br> number 7: 2.9352 <div class=highlight><pre><span></span><code><span class=n>iter_test</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
    <span class=n>iter_test</span> <span class=o>+=</span> <span class=mi>1</span>
    <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>
    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>iter_test</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;OUTPUTS&#39;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=p>:])</span>
    <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</code></pre></div></p> </div> <div class=highlight><pre><span></span><code>OUTPUTS
tensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639,
         2.9352, -0.1552,  0.8852])
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Printing prediction output</p> <p>Because our output is of size 100 (our batch size), our prediction size would also of the size 100.</p> <div class=highlight><pre><span></span><code><span class=n>iter_test</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
    <span class=n>iter_test</span> <span class=o>+=</span> <span class=mi>1</span>
    <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>
    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>
    <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>iter_test</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;PREDICTION&#39;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>predicted</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>PREDICTION</span>
<span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>100</span><span class=p>])</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Print prediction value</p> <p>We are printing our prediction which as verified above, should be digit 7.</p> <div class=highlight><pre><span></span><code><span class=n>iter_test</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
    <span class=n>iter_test</span> <span class=o>+=</span> <span class=mi>1</span>
    <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>
    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>
    <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>iter_test</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;PREDICTION&#39;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>predicted</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>PREDICTION</span>
<span class=n>tensor</span><span class=p>(</span><span class=mi>7</span><span class=p>)</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Print prediction, label and label size</p> <p>We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7!</p> <div class=highlight><pre><span></span><code><span class=n>iter_test</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
    <span class=n>iter_test</span> <span class=o>+=</span> <span class=mi>1</span>
    <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>
    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>
    <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>iter_test</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;PREDICTION&#39;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>predicted</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>

        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;LABEL SIZE&#39;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>

        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;LABEL FOR IMAGE 0&#39;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>labels</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>PREDICTION</span>
<span class=n>tensor</span><span class=p>(</span><span class=mi>7</span><span class=p>)</span>

<span class=n>LABEL</span> <span class=n>SIZE</span>
<span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>100</span><span class=p>])</span>

<span class=n>LABEL</span> <span class=n>FOR</span> <span class=n>IMAGE</span> <span class=mi>0</span>
<span class=n>tensor</span><span class=p>(</span><span class=mi>7</span><span class=p>)</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Print second prediction and ground truth</p> <p>Again, the prediction is correct. Naturally, as our model is quite competent in this simple task.</p> <div class=highlight><pre><span></span><code><span class=n>iter_test</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
    <span class=n>iter_test</span> <span class=o>+=</span> <span class=mi>1</span>
    <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>
    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>
    <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

    <span class=k>if</span> <span class=n>iter_test</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;PREDICTION&#39;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>predicted</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>

        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;LABEL SIZE&#39;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>

        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;LABEL FOR IMAGE 1&#39;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>labels</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>PREDICTION</span>
<span class=n>tensor</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>

<span class=n>LABEL</span> <span class=n>SIZE</span>
<span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>100</span><span class=p>])</span>

<span class=n>LABEL</span> <span class=n>FOR</span> <span class=n>IMAGE</span> <span class=mi>1</span>
<span class=n>tensor</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Print accuracy</p> <p>Now we know what each object represents, we can understand how we arrived at our accuracy numbers.</p> <p>One last thing to note is that <code>correct.item()</code> has this syntax is because <code>correct</code> is a PyTorch tensor and to get the value to compute with <code>total</code> which is an integer, we need to do this. <div class=highlight><pre><span></span><code><span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>iter_test</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
    <span class=n>iter_test</span> <span class=o>+=</span> <span class=mi>1</span>
    <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>
    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>
    <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

    <span class=c1># Total number of labels</span>
    <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

    <span class=c1># Total correct predictions</span>
    <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

<span class=n>accuracy</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=p>(</span><span class=n>correct</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>/</span> <span class=n>total</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=n>accuracy</span><span class=p>)</span>
</code></pre></div></p> </div> <div class=highlight><pre><span></span><code><span class=mf>82.94</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Explanation of Python's .sum() function</p> <p>Python's .sum() function allows you to do a comparison between two matrices and sum the ones that return <code>True</code> or in our case, those predictions that match actual labels (correct predictions).</p> <div class=highlight><pre><span></span><code><span class=c1># Explaining .sum() python built-in function</span>
<span class=c1># correct += (predicted == labels).sum()</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
<span class=n>a</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>10</span><span class=p>))</span>
<span class=nb>print</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
<span class=n>b</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>10</span><span class=p>))</span>
<span class=nb>print</span><span class=p>(</span><span class=n>b</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=n>a</span> <span class=o>==</span> <span class=n>b</span><span class=p>)</span>

<span class=nb>print</span><span class=p>((</span><span class=n>a</span> <span class=o>==</span> <span class=n>b</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>())</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=c1># matrix a</span>
<span class=p>[</span><span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span><span class=p>]</span>

<span class=c1># matrix b</span>
<span class=p>[</span><span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span> <span class=mf>1.</span><span class=p>]</span>

<span class=c1># boolean array</span>
<span class=p>[</span> <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span><span class=p>]</span>

<span class=c1># number of elementswhere a matches b</span>
<span class=mi>10</span>
</code></pre></div> <h4 id=saving-model>Saving Model<a class=headerlink href=#saving-model title="Permanent link">&para;</a></h4> <div class="admonition note"> <p class=admonition-title>Saving PyTorch model</p> <p>This is how you save your model. Feel free to just change <code>save_model = True</code> to save your model <div class=highlight><pre><span></span><code><span class=n>save_model</span> <span class=o>=</span> <span class=kc>False</span>
<span class=k>if</span> <span class=n>save_model</span> <span class=ow>is</span> <span class=kc>True</span><span class=p>:</span>
    <span class=c1># Saves only parameters</span>
    <span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span> <span class=s1>&#39;awesome_model.pkl&#39;</span><span class=p>)</span>
</code></pre></div></p> </div> <h2 id=building-a-logistic-regression-model-with-pytorch-gpu>Building a Logistic Regression Model with PyTorch (GPU)<a class=headerlink href=#building-a-logistic-regression-model-with-pytorch-gpu title="Permanent link">&para;</a></h2> <div class="admonition note"> <p class=admonition-title>CPU version</p> <p>The usual 7-step process, getting repetitive by now which we like. </p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>3000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>LogisticRegressionModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>LogisticRegressionModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>LogisticRegressionModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>


<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.001</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images as Variable</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>
        <span class=n>labels</span> <span class=o>=</span> <span class=n>labels</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=c1># 100 x 10</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=c1># 100 x 1</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1># Total correct predictions</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=n>correct</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Iteration: </span><span class=si>{}</span><span class=s1>. Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>Iteration</span><span class=p>:</span> <span class=mf>500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.876196026802063</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>64.44</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.5153584480285645</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>75.68</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.3521136045455933</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>78.98</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.2136967182159424</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>80.95</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.0934826135635376</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>81.97</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>3000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.024120569229126</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>82.49</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>GPU version</p> <p>2 things must be on GPU <br>- <code>model</code> <br>- <code>tensors</code></p> <p>Remember step 4 and 7 will be affected and this will be the same for all model building moving forward.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>3000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>LogisticRegressionModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>LogisticRegressionModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>LogisticRegressionModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=c1>#######################</span>
<span class=c1>#  USE GPU FOR MODEL  #</span>
<span class=c1>#######################</span>

<span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&quot;cuda:0&quot;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&quot;cpu&quot;</span><span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>


<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.001</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>

        <span class=c1>#######################</span>
        <span class=c1>#  USE GPU FOR MODEL  #</span>
        <span class=c1>#######################</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
        <span class=n>labels</span> <span class=o>=</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1>#######################</span>
                <span class=c1>#  USE GPU FOR MODEL  #</span>
                <span class=c1>#######################</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1>#######################</span>
                <span class=c1>#  USE GPU FOR MODEL  #</span>
                <span class=c1>#######################</span>
                <span class=c1># Total correct predictions</span>
                <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span>
                    <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span> <span class=o>==</span> <span class=n>labels</span><span class=o>.</span><span class=n>cpu</span><span class=p>())</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
                <span class=k>else</span><span class=p>:</span>
                    <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=n>correct</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Iteration: </span><span class=si>{}</span><span class=s1>. Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>Iteration</span><span class=p>:</span> <span class=mf>500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.8571407794952393</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>68.99</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.5415704250335693</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>75.86</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.2755383253097534</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>78.92</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.2468739748001099</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>80.72</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.0708973407745361</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>81.73</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>3000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>1.0359245538711548</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>82.74</span>
</code></pre></div> <h2 id=summary>Summary<a class=headerlink href=#summary title="Permanent link">&para;</a></h2> <p>We've learnt to...</p> <div class="admonition success"> <p class=admonition-title>Success</p> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> <strong>Logistic regression</strong> basics</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> <strong>Problems</strong> of <strong>linear regression</strong></li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> <strong>In-depth</strong> Logistic Regression<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Get logits</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Get softmax</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Get cross-entropy loss</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> <strong>Aim</strong>: reduce cross-entropy loss</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Built a <strong>logistic regression model</strong> in <strong>CPU and GPU</strong><ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Step 1: Load Dataset</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Step 2: Make Dataset Iterable</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Step 3: Create Model Class</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Step 4: Instantiate Model Class</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Step 5: Instantiate Loss Class</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Step 6: Instantiate Optimizer Class</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Step 7: Train Model</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Important things to be on <strong>GPU</strong><ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> <code>model</code></li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> <code>tensors with gradients</code></li> </ul> </li> </ul> </div> <h2 id=citation>Citation<a class=headerlink href=#citation title="Permanent link">&para;</a></h2> <p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p><a href=https://zenodo.org/badge/latestdoi/139945544><img alt=DOI src=https://zenodo.org/badge/139945544.svg></a></p> </article> </div> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 Disponha </div> Todo o conteúdo deste site está publicado sob a licença <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank rel=noopener> Creative Commons CC BY-SA 4.0 Brasil </a> </div> <div class=md-footer-social> <a href=https://github.com/andhremattos target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://facebook.com/dhematos target=_blank rel=noopener title=facebook.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg> </a> <a href=https://instagram.com/dhematos target=_blank rel=noopener title=instagram.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../../../assets/javascripts/vendor.de50e36d.min.js></script> <script src=../../../assets/javascripts/bundle.fc9c3121.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copiar para \u00e1rea de transfer\u00eancia", "clipboard.copied": "Copiado para \u00e1rea de transfer\u00eancia", "search.config.lang": "pt", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Digite para iniciar a busca", "search.result.none": "Nenhum resultado encontrado", "search.result.one": "1 resultado encontrado", "search.result.other": "# resultados encontrados"}</script> <script>
        app = initialize({
          base: "../../..",
          features: ["tabs"],
          search: Object.assign({
            worker:"../../../assets/javascripts/worker/search.a68abb33.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> <script src=../../../javascripts/config.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> </body> </html>