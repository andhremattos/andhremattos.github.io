<!doctype html><html lang=pt class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Plataforma para disponibilização cursos de cálculo."><link href=https:///disponha.com/pre_calculo/conjuntos_numericos/lr_scheduling/ rel=canonical><meta name=author content="Carlos André"><link rel="shortcut icon" href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.1.2, mkdocs-material-5.4.0"><title>Learning Rate Scheduling - disponha</title><link rel=stylesheet href=../../../assets/stylesheets/main.545621a7.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.36d1b78f.min.css><meta name=theme-color content=#009688><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Helvetica:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Helvetica",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","None","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=teal data-md-color-accent=teal> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#learning-rate-scheduling class=md-skip> Ir para o conteúdo </a> </div> <!-- 
    <div data-md-component="announce">
      
    </div>
  --> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https:/disponha.com/ title=disponha class="md-header-nav__button md-logo" aria-label=disponha> <?xml version="1.0" encoding="UTF-8"?> <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><!-- Creator: CorelDRAW X7 --><svg xmlns=http://www.w3.org/2000/svg xml:space=preserve width=9000px height=9000px version=1.1 style="shape-rendering:geometricPrecision; text-rendering:geometricPrecision; image-rendering:optimizeQuality; fill-rule:evenodd; clip-rule:evenodd" viewbox="0 0 9000 9000" xmlns:xlink=http://www.w3.org/1999/xlink> <defs> <style type=text/css>
   <![CDATA[
    .str0 {stroke:#2E72A4;stroke-width:118.11}
    .fil0 {fill:#FEFEFE}
    .fil1 {fill:#2E72A4}
   ]]>
  </style> </defs> <g id=Camada_x0020_1> <metadata id=CorelCorpID_0Corel-Layer /> <g id=_1793748728448> <circle cx=4472 cy=4507 r=4281 class="fil0 str0"/> <path class="fil1 str0" d="M2045 5171c120,204 210,379 402,449 314,115 1000,-177 1339,-330 79,-35 179,-68 254,-120 41,-13 94,-46 135,-68l951 -479c363,-177 729,-363 1095,-521 186,-80 370,-180 558,-244 465,-158 1366,-413 1814,-369l1 -59c-328,-34 -853,93 -1189,181 -431,113 -680,200 -1103,395 -706,324 -1401,670 -2099,1021 -397,199 -1256,618 -1669,556 -218,-33 -320,-195 -449,-416 -47,-80 -137,-241 -177,-328 -49,-108 -114,-212 -179,-311 -379,-575 -831,-826 -1274,-1083l-119 -63 -10 46 247 149c311,184 648,381 929,716 107,128 198,250 291,408 61,104 217,424 252,470z"/> </g> </g> </svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> disponha </span> <span class="md-header-nav__topic md-ellipsis"> Learning Rate Scheduling </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Buscar placeholder=Buscar autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <!-- 

<a href="https://github.com/andhremattos/" title="Ir ao repositório" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    esead
  </div>
</a>
--> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <!--
      <a href="../../.." class="md-tabs__link md-tabs__link--active">
        Home
      </a>
    --> <a href=/explore/ class="md-tabs__link md-tabs__link--active"> Explore e-books </a> </li> <!--
    <li class="md-tabs__item">
      
        <a href="../../../sobre/" class="md-tabs__link">
          Sobre
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../explore/" class="md-tabs__link">
          Explore
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../app_web_ebook/servico/" class="md-tabs__link">
          App Web ebook
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../diagramacao/servico/" class="md-tabs__link">
          Diagramação
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../ciencia_de_dados/servico/" class="md-tabs__link">
          Ciência de dados
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../aprendacalculo/" class="md-tabs__link">
          Aprenda Cálculo
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../cursolatex/" class="md-tabs__link">
          Curso de LaTeX
        </a>
      
    </li>--> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https:/disponha.com/ title=disponha class="md-nav__button md-logo" aria-label=disponha> <?xml version="1.0" encoding="UTF-8"?> <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><!-- Creator: CorelDRAW X7 --><svg xmlns=http://www.w3.org/2000/svg xml:space=preserve width=9000px height=9000px version=1.1 style="shape-rendering:geometricPrecision; text-rendering:geometricPrecision; image-rendering:optimizeQuality; fill-rule:evenodd; clip-rule:evenodd" viewbox="0 0 9000 9000" xmlns:xlink=http://www.w3.org/1999/xlink> <defs> <style type=text/css>
   <![CDATA[
    .str0 {stroke:#2E72A4;stroke-width:118.11}
    .fil0 {fill:#FEFEFE}
    .fil1 {fill:#2E72A4}
   ]]>
  </style> </defs> <g id=Camada_x0020_1> <metadata id=CorelCorpID_0Corel-Layer /> <g id=_1793748728448> <circle cx=4472 cy=4507 r=4281 class="fil0 str0"/> <path class="fil1 str0" d="M2045 5171c120,204 210,379 402,449 314,115 1000,-177 1339,-330 79,-35 179,-68 254,-120 41,-13 94,-46 135,-68l951 -479c363,-177 729,-363 1095,-521 186,-80 370,-180 558,-244 465,-158 1366,-413 1814,-369l1 -59c-328,-34 -853,93 -1189,181 -431,113 -680,200 -1103,395 -706,324 -1401,670 -2099,1021 -397,199 -1256,618 -1669,556 -218,-33 -320,-195 -449,-416 -47,-80 -137,-241 -177,-328 -49,-108 -114,-212 -179,-311 -379,-575 -831,-826 -1274,-1083l-119 -63 -10 46 247 149c311,184 648,381 929,716 107,128 198,250 291,408 61,104 217,424 252,470z"/> </g> </g> </svg> </a> disponha </label> <div class=md-nav__source> <!-- 

<a href="https://github.com/andhremattos/" title="Ir ao repositório" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    esead
  </div>
</a>
--> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. title=Home class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2> <label class=md-nav__link for=nav-2> Sobre <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Sobre data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Sobre </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../sobre/ title=Home class=md-nav__link> Home </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Explore <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Explore data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Explore </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../explore/ title=E-books class=md-nav__link> E-books </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../apoie/ title=Apoie class=md-nav__link> Apoie </a> </li> <li class=md-nav__item> <a href=../../../consultoria/ title=Consultoria class=md-nav__link> Consultoria </a> </li> <li class=md-nav__item> <a href=../../../politica_privacidade/ title=Privacidade class=md-nav__link> Privacidade </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7 type=checkbox id=nav-7> <label class=md-nav__link for=nav-7> App Web ebook <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="App Web ebook" data-md-level=1> <label class=md-nav__title for=nav-7> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> App Web ebook </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../app_web_ebook/servico/ title=Serviço class=md-nav__link> Serviço </a> </li> <li class=md-nav__item> <a href=../../../app_web_ebook/amostra/ title="App Web ebook - amostra" class=md-nav__link> App Web ebook - amostra </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-8 type=checkbox id=nav-8> <label class=md-nav__link for=nav-8> Diagramação <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Diagramação data-md-level=1> <label class=md-nav__title for=nav-8> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Diagramação </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../diagramacao/servico/ title=Serviço class=md-nav__link> Serviço </a> </li> <li class=md-nav__item> <a href=../../../diagramacao/amostra-diagramacao/ title=Amostra class=md-nav__link> Amostra </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-9 type=checkbox id=nav-9> <label class=md-nav__link for=nav-9> Ciência de dados <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Ciência de dados" data-md-level=1> <label class=md-nav__title for=nav-9> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Ciência de dados </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../ciencia_de_dados/servico/ title=Serviço class=md-nav__link> Serviço </a> </li> <li class=md-nav__item> <a href=../../../ciencia_de_dados/amostra-1/ title="Amostra 1" class=md-nav__link> Amostra 1 </a> </li> <li class=md-nav__item> <a href=../../../ciencia_de_dados/amostra-2/ title="Amostra 2" class=md-nav__link> Amostra 2 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-10 type=checkbox id=nav-10> <label class=md-nav__link for=nav-10> Aprenda Cálculo <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Aprenda Cálculo" data-md-level=1> <label class=md-nav__title for=nav-10> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Aprenda Cálculo </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../aprendacalculo/ title=Bem-Vindo(a)s class=md-nav__link> Bem-Vindo(a)s </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/pre_calculo/ title=Pré-Cálculo class=md-nav__link> Pré-Cálculo </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_i/ title="Cálculo I" class=md-nav__link> Cálculo I </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_ii/ title="Cálculo II" class=md-nav__link> Cálculo II </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_iii/ title="Cálculo III" class=md-nav__link> Cálculo III </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_iv/ title="Cálculo IV" class=md-nav__link> Cálculo IV </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_numerico/ title="C. Numérico" class=md-nav__link> C. Numérico </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-11 type=checkbox id=nav-11> <label class=md-nav__link for=nav-11> Curso de LaTeX <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Curso de LaTeX" data-md-level=1> <label class=md-nav__title for=nav-11> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Curso de LaTeX </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../cursolatex/ title=Bem-Vindo(a)s class=md-nav__link> Bem-Vindo(a)s </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/programas/ title=Programas class=md-nav__link> Programas </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/latex_online/ title="LaTeX online" class=md-nav__link> LaTeX online </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_1/ title="Parte 1" class=md-nav__link> Parte 1 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_2/ title="Parte 2" class=md-nav__link> Parte 2 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_3/ title="Parte 3" class=md-nav__link> Parte 3 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_4/ title="Parte 4" class=md-nav__link> Parte 4 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_5/ title="Parte 5" class=md-nav__link> Parte 5 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_6/ title="Parte 6" class=md-nav__link> Parte 6 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_7/ title="Parte 7" class=md-nav__link> Parte 7 </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=Índice> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Índice </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#optimization-algorithm-mini-batch-stochastic-gradient-descent-sgd class=md-nav__link> Optimization Algorithm: Mini-batch Stochastic Gradient Descent (SGD) </a> </li> <li class=md-nav__item> <a href=#learning-intuition-recap class=md-nav__link> Learning Intuition Recap </a> </li> <li class=md-nav__item> <a href=#learning-rate-pointers class=md-nav__link> Learning Rate Pointers </a> </li> <li class=md-nav__item> <a href=#need-for-learning-rate-schedules class=md-nav__link> Need for Learning Rate Schedules </a> </li> <li class=md-nav__item> <a href=#top-basic-learning-rate-schedules class=md-nav__link> Top Basic Learning Rate Schedules </a> <nav class=md-nav aria-label="Top Basic Learning Rate Schedules"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#step-wise-learning-rate-decay class=md-nav__link> Step-wise Learning Rate Decay </a> <nav class=md-nav aria-label="Step-wise Learning Rate Decay"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#step-wise-decay-every-epoch class=md-nav__link> Step-wise Decay: Every Epoch </a> </li> <li class=md-nav__item> <a href=#step-wise-decay-every-2-epochs class=md-nav__link> Step-wise Decay: Every 2 Epochs </a> </li> <li class=md-nav__item> <a href=#step-wise-decay-every-epoch-larger-gamma class=md-nav__link> Step-wise Decay: Every Epoch, Larger Gamma </a> </li> <li class=md-nav__item> <a href=#pointers-on-step-wise-decay class=md-nav__link> Pointers on Step-wise Decay </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#reduce-on-loss-plateau-decay class=md-nav__link> Reduce on Loss Plateau Decay </a> <nav class=md-nav aria-label="Reduce on Loss Plateau Decay"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#reduce-on-loss-plateau-decay-patience0-factor01 class=md-nav__link> Reduce on Loss Plateau Decay, Patience=0, Factor=0.1 </a> </li> <li class=md-nav__item> <a href=#reduce-on-loss-plateau-decay-patience0-factor05 class=md-nav__link> Reduce on Loss Plateau Decay, Patience=0, Factor=0.5 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#pointers-on-reduce-on-loss-pleateau-decay class=md-nav__link> Pointers on Reduce on Loss Pleateau Decay </a> </li> <li class=md-nav__item> <a href=#summary class=md-nav__link> Summary </a> </li> <li class=md-nav__item> <a href=#citation class=md-nav__link> Citation </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=learning-rate-scheduling>Learning Rate Scheduling<a class=headerlink href=#learning-rate-scheduling title="Permanent link">&para;</a></h1> <div class="admonition tip"> <p class=admonition-title>Run Jupyter Notebook</p> <p>You can run the code for this section in this <a href=https://github.com/ritchieng/deep-learning-wizard/blob/master/docs/deep_learning/boosting_models_pytorch/lr_scheduling.md>jupyter notebook link</a>.</p> </div> <h2 id=optimization-algorithm-mini-batch-stochastic-gradient-descent-sgd>Optimization Algorithm: Mini-batch Stochastic Gradient Descent (SGD)<a class=headerlink href=#optimization-algorithm-mini-batch-stochastic-gradient-descent-sgd title="Permanent link">&para;</a></h2> <ul> <li>We will be using mini-batch gradient descent in all our examples here when scheduling our learning rate</li> <li>Combination of batch gradient descent &amp; stochastic gradient descent<ul> <li><span><span class=MathJax_Preview>\theta = \theta - \eta \cdot \nabla J(\theta, x^{i: i+n}, y^{i:i+n})</span><script type=math/tex>\theta = \theta - \eta \cdot  \nabla J(\theta, x^{i: i+n}, y^{i:i+n})</script></span></li> </ul> </li> <li>Characteristics<ul> <li>Compute the gradient of the lost function w.r.t. parameters for <strong>n sets of training sample (n input and n label)</strong>, <span><span class=MathJax_Preview>\nabla J(\theta, x^{i: i+n}, y^{i:i+n})</span><script type=math/tex>\nabla J(\theta, x^{i: i+n}, y^{i:i+n})</script></span></li> <li>Use this to update our parameters at every iteration</li> </ul> </li> <li>Typically in deep learning, some variation of mini-batch gradient is used where the batch size is a hyperparameter to be determined</li> </ul> <h2 id=learning-intuition-recap>Learning Intuition Recap<a class=headerlink href=#learning-intuition-recap title="Permanent link">&para;</a></h2> <ul> <li>Learning process<ul> <li>Original parameters <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> given input, get output <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> compare with labels <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> get loss with comparison of input/output <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> get gradients of loss w.r.t parameters <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> <strong>update parameters so model can churn output closer to labels</strong> <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> repeat</li> </ul> </li> <li>For a detailed mathematical account of how this works and how to implement from scratch in Python and PyTorch, you can read our <a href=https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/ >forward- and back-propagation and gradient descent post</a>.</li> </ul> <h2 id=learning-rate-pointers>Learning Rate Pointers<a class=headerlink href=#learning-rate-pointers title="Permanent link">&para;</a></h2> <p><img alt src=../images/lr1.png></p> <ul> <li><strong>Update parameters so model can churn output closer to labels, lower loss</strong><ul> <li><span><span class=MathJax_Preview>\theta = \theta - \eta \cdot \nabla J(\theta, x^{i: i+n}, y^{i:i+n})</span><script type=math/tex>\theta = \theta - \eta \cdot  \nabla J(\theta, x^{i: i+n}, y^{i:i+n})</script></span></li> </ul> </li> <li>If we set <span><span class=MathJax_Preview>\eta</span><script type=math/tex>\eta</script></span> to be a <strong>large value</strong> <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> learn too much (rapid learning)<ul> <li>Unable to converge to a good local minima (unable to effectively gradually decrease your loss, overshoot the local lowest value)</li> </ul> </li> <li>If we set <span><span class=MathJax_Preview>\eta</span><script type=math/tex>\eta</script></span> to be a <strong>small value</strong> <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> learn too little (slow learning)<ul> <li>May take too long or unable to convert to a good local minima</li> </ul> </li> </ul> <h2 id=need-for-learning-rate-schedules>Need for Learning Rate Schedules<a class=headerlink href=#need-for-learning-rate-schedules title="Permanent link">&para;</a></h2> <ul> <li>Benefits<ul> <li>Converge faster</li> <li>Higher accuracy <img alt src=../images/lr2.png></li> </ul> </li> </ul> <h2 id=top-basic-learning-rate-schedules>Top Basic Learning Rate Schedules<a class=headerlink href=#top-basic-learning-rate-schedules title="Permanent link">&para;</a></h2> <ol> <li>Step-wise Decay </li> <li>Reduce on Loss Plateau Decay</li> </ol> <h3 id=step-wise-learning-rate-decay>Step-wise Learning Rate Decay<a class=headerlink href=#step-wise-learning-rate-decay title="Permanent link">&para;</a></h3> <h4 id=step-wise-decay-every-epoch>Step-wise Decay: Every Epoch<a class=headerlink href=#step-wise-decay-every-epoch title="Permanent link">&para;</a></h4> <ul> <li>At every epoch,<ul> <li><span><span class=MathJax_Preview>\eta_t = \eta_{t-1}\gamma</span><script type=math/tex>\eta_t = \eta_{t-1}\gamma</script></span></li> <li><span><span class=MathJax_Preview>\gamma = 0.1</span><script type=math/tex>\gamma = 0.1</script></span></li> </ul> </li> <li>Optimization Algorithm 4: SGD Nesterov<ul> <li>Modification of SGD Momentum <ul> <li><span><span class=MathJax_Preview>v_t = \gamma v_{t-1} + \eta \cdot \nabla J(\theta - \gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})</span><script type=math/tex>v_t = \gamma v_{t-1} + \eta \cdot  \nabla J(\theta - \gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})</script></span></li> <li><span><span class=MathJax_Preview>\theta = \theta - v_t</span><script type=math/tex>\theta = \theta - v_t</script></span></li> </ul> </li> </ul> </li> <li>Practical example<ul> <li>Given <span><span class=MathJax_Preview>\eta_t = 0.1</span><script type=math/tex>\eta_t = 0.1</script></span> and $ \gamma = 0.01$</li> <li>Epoch 0: <span><span class=MathJax_Preview>\eta_t = 0.1</span><script type=math/tex>\eta_t = 0.1</script></span></li> <li>Epoch 1: <span><span class=MathJax_Preview>\eta_{t+1} = 0.1 (0.1) = 0.01</span><script type=math/tex>\eta_{t+1} = 0.1 (0.1) =  0.01</script></span></li> <li>Epoch 2: <span><span class=MathJax_Preview>\eta_{t+2} = 0.1 (0.1)^2 = 0.001</span><script type=math/tex>\eta_{t+2} = 0.1 (0.1)^2 =  0.001</script></span></li> <li>Epoch n: <span><span class=MathJax_Preview>\eta_{t+n} = 0.1 (0.1)^n</span><script type=math/tex>\eta_{t+n} = 0.1 (0.1)^n</script></span></li> </ul> </li> </ul> <div class="admonition note"> <p class=admonition-title>Code for step-wise learning rate decay at every epoch</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>

<span class=c1># Set seed</span>
<span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Where to add a new import</span>
<span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>StepLR</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>3000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>FeedforwardNeuralNetModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Linear function</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> 
        <span class=c1># Non-linearity</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
        <span class=c1># Linear function (readout)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Linear function</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=c1># Linear function (readout)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>


<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>nesterov</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=c1># step_size: at how many multiples of epoch you decay</span>
<span class=c1># step_size = 1, after every 1 epoch, new_lr = lr*gamma </span>
<span class=c1># step_size = 2, after every 2 epoch, new_lr = lr*gamma </span>

<span class=c1># gamma = decaying factor</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=c1># Decay Learning Rate</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
    <span class=c1># Print Learning Rate</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch:&#39;</span><span class=p>,</span> <span class=n>epoch</span><span class=p>,</span><span class=s1>&#39;LR:&#39;</span><span class=p>,</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>get_lr</span><span class=p>())</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1># Total correct predictions</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Iteration: </span><span class=si>{}</span><span class=s1>. Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>Epoch</span><span class=p>:</span> <span class=mi>0</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.1</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.15292978286743164</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>96</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>1</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.010000000000000002</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.1207798570394516</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>2</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.0010000000000000002</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.12287932634353638</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>3</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.00010000000000000003</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.05614742264151573</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>4</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>1.0000000000000003e-05</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.06775809079408646</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>3000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.03737065941095352</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
</code></pre></div> <h4 id=step-wise-decay-every-2-epochs>Step-wise Decay: Every 2 Epochs<a class=headerlink href=#step-wise-decay-every-2-epochs title="Permanent link">&para;</a></h4> <ul> <li>At every 2 epoch,<ul> <li><span><span class=MathJax_Preview>\eta_t = \eta_{t-1}\gamma</span><script type=math/tex>\eta_t = \eta_{t-1}\gamma</script></span></li> <li><span><span class=MathJax_Preview>\gamma = 0.1</span><script type=math/tex>\gamma = 0.1</script></span></li> </ul> </li> <li>Optimization Algorithm 4: SGD Nesterov<ul> <li>Modification of SGD Momentum <ul> <li><span><span class=MathJax_Preview>v_t = \gamma v_{t-1} + \eta \cdot \nabla J(\theta - \gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})</span><script type=math/tex>v_t = \gamma v_{t-1} + \eta \cdot  \nabla J(\theta - \gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})</script></span></li> <li><span><span class=MathJax_Preview>\theta = \theta - v_t</span><script type=math/tex>\theta = \theta - v_t</script></span></li> </ul> </li> </ul> </li> <li>Practical example<ul> <li>Given <span><span class=MathJax_Preview>\eta_t = 0.1</span><script type=math/tex>\eta_t = 0.1</script></span> and <span><span class=MathJax_Preview>\gamma = 0.01</span><script type=math/tex>\gamma = 0.01</script></span></li> <li>Epoch 0: <span><span class=MathJax_Preview>\eta_t = 0.1</span><script type=math/tex>\eta_t = 0.1</script></span></li> <li>Epoch 1: <span><span class=MathJax_Preview>\eta_{t+1} = 0.1</span><script type=math/tex>\eta_{t+1} = 0.1</script></span></li> <li>Epoch 2: <span><span class=MathJax_Preview>\eta_{t+2} = 0.1 (0.1) = 0.01</span><script type=math/tex>\eta_{t+2} = 0.1 (0.1) =  0.01</script></span></li> </ul> </li> </ul> <div class="admonition note"> <p class=admonition-title>Code for step-wise learning rate decay at every 2 epoch</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>

<span class=c1># Set seed</span>
<span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Where to add a new import</span>
<span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>StepLR</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>3000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>FeedforwardNeuralNetModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Linear function</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> 
        <span class=c1># Non-linearity</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
        <span class=c1># Linear function (readout)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Linear function</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=c1># Linear function (readout)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>nesterov</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=c1># step_size: at how many multiples of epoch you decay</span>
<span class=c1># step_size = 1, after every 2 epoch, new_lr = lr*gamma </span>
<span class=c1># step_size = 2, after every 2 epoch, new_lr = lr*gamma </span>

<span class=c1># gamma = decaying factor</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=c1># Decay Learning Rate</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
    <span class=c1># Print Learning Rate</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch:&#39;</span><span class=p>,</span> <span class=n>epoch</span><span class=p>,</span><span class=s1>&#39;LR:&#39;</span><span class=p>,</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>get_lr</span><span class=p>())</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images as Variable</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1># Total correct predictions</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Iteration: </span><span class=si>{}</span><span class=s1>. Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>Epoch</span><span class=p>:</span> <span class=mi>0</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.1</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.15292978286743164</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>96</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>1</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.1</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.11253029108047485</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>96</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>2</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.010000000000000002</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.14498558640480042</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>3</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.010000000000000002</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.03691177815198898</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>4</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.0010000000000000002</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.03511016443371773</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>3000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.029424520209431648</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
</code></pre></div> <h4 id=step-wise-decay-every-epoch-larger-gamma>Step-wise Decay: Every Epoch, Larger Gamma<a class=headerlink href=#step-wise-decay-every-epoch-larger-gamma title="Permanent link">&para;</a></h4> <ul> <li>At every epoch,<ul> <li><span><span class=MathJax_Preview>\eta_t = \eta_{t-1}\gamma</span><script type=math/tex>\eta_t = \eta_{t-1}\gamma</script></span></li> <li><span><span class=MathJax_Preview>\gamma = 0.96</span><script type=math/tex>\gamma = 0.96</script></span></li> </ul> </li> <li>Optimization Algorithm 4: SGD Nesterov<ul> <li>Modification of SGD Momentum <ul> <li><span><span class=MathJax_Preview>v_t = \gamma v_{t-1} + \eta \cdot \nabla J(\theta - \gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})</span><script type=math/tex>v_t = \gamma v_{t-1} + \eta \cdot  \nabla J(\theta - \gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})</script></span></li> <li><span><span class=MathJax_Preview>\theta = \theta - v_t</span><script type=math/tex>\theta = \theta - v_t</script></span></li> </ul> </li> </ul> </li> <li>Practical example<ul> <li>Given <span><span class=MathJax_Preview>\eta_t = 0.1</span><script type=math/tex>\eta_t = 0.1</script></span> and <span><span class=MathJax_Preview>\gamma = 0.96</span><script type=math/tex>\gamma = 0.96</script></span></li> <li>Epoch 1: <span><span class=MathJax_Preview>\eta_t = 0.1</span><script type=math/tex>\eta_t = 0.1</script></span></li> <li>Epoch 2: <span><span class=MathJax_Preview>\eta_{t+1} = 0.1 (0.96) = 0.096</span><script type=math/tex>\eta_{t+1} = 0.1 (0.96) =  0.096</script></span></li> <li>Epoch 3: <span><span class=MathJax_Preview>\eta_{t+2} = 0.1 (0.96)^2 = 0.092</span><script type=math/tex>\eta_{t+2} = 0.1 (0.96)^2 =  0.092</script></span></li> <li>Epoch n: <span><span class=MathJax_Preview>\eta_{t+n} = 0.1 (0.96)^n</span><script type=math/tex>\eta_{t+n} = 0.1 (0.96)^n</script></span></li> </ul> </li> </ul> <div class="admonition note"> <p class=admonition-title>Code for step-wise learning rate decay at every epoch with larger gamma</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>

<span class=c1># Set seed</span>
<span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Where to add a new import</span>
<span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>StepLR</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>3000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>FeedforwardNeuralNetModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Linear function</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> 
        <span class=c1># Non-linearity</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
        <span class=c1># Linear function (readout)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Linear function</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=c1># Linear function (readout)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>


<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>nesterov</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=c1># step_size: at how many multiples of epoch you decay</span>
<span class=c1># step_size = 1, after every 2 epoch, new_lr = lr*gamma </span>
<span class=c1># step_size = 2, after every 2 epoch, new_lr = lr*gamma </span>

<span class=c1># gamma = decaying factor</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.96</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=c1># Decay Learning Rate</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
    <span class=c1># Print Learning Rate</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch:&#39;</span><span class=p>,</span> <span class=n>epoch</span><span class=p>,</span><span class=s1>&#39;LR:&#39;</span><span class=p>,</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>get_lr</span><span class=p>())</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images as Variable</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1># Total correct predictions</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Iteration: </span><span class=si>{}</span><span class=s1>. Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>Epoch</span><span class=p>:</span> <span class=mi>0</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.1</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.15292978286743164</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>96</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>1</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.1</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.11253029108047485</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>96</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>2</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.096</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>1500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.11864850670099258</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>3</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.096</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.030942382290959358</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
<span class=n>Epoch</span><span class=p>:</span> <span class=mi>4</span> <span class=n>LR</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.09216</span><span class=p>]</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>2500.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.04521659016609192</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
<span class=n>Iteration</span><span class=p>:</span> <span class=mf>3000.</span> <span class=n>Loss</span><span class=p>:</span> <span class=mf>0.027839098125696182</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mi>97</span>
</code></pre></div> <h4 id=pointers-on-step-wise-decay>Pointers on Step-wise Decay<a class=headerlink href=#pointers-on-step-wise-decay title="Permanent link">&para;</a></h4> <ul> <li>You would want to decay your LR gradually when you're training more epochs<ul> <li>Converge too fast, to a crappy loss/accuracy, if you decay rapidly</li> </ul> </li> <li>To decay slower<ul> <li>Larger <span><span class=MathJax_Preview>\gamma</span><script type=math/tex>\gamma</script></span></li> <li>Larger interval of decay</li> </ul> </li> </ul> <h3 id=reduce-on-loss-plateau-decay>Reduce on Loss Plateau Decay<a class=headerlink href=#reduce-on-loss-plateau-decay title="Permanent link">&para;</a></h3> <h4 id=reduce-on-loss-plateau-decay-patience0-factor01>Reduce on Loss Plateau Decay, Patience=0, Factor=0.1<a class=headerlink href=#reduce-on-loss-plateau-decay-patience0-factor01 title="Permanent link">&para;</a></h4> <ul> <li>Reduce learning rate whenever loss plateaus<ul> <li>Patience: number of epochs with no improvement after which learning rate will be reduced<ul> <li>Patience = 0</li> </ul> </li> <li>Factor: multiplier to decrease learning rate, <span><span class=MathJax_Preview>lr = lr*factor = \gamma</span><script type=math/tex>lr = lr*factor = \gamma</script></span><ul> <li>Factor = 0.1</li> </ul> </li> </ul> </li> <li>Optimization Algorithm: SGD Nesterov<ul> <li>Modification of SGD Momentum <ul> <li><span><span class=MathJax_Preview>v_t = \gamma v_{t-1} + \eta \cdot \nabla J(\theta - \gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})</span><script type=math/tex>v_t = \gamma v_{t-1} + \eta \cdot  \nabla J(\theta - \gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})</script></span></li> <li><span><span class=MathJax_Preview>\theta = \theta - v_t</span><script type=math/tex>\theta = \theta - v_t</script></span></li> </ul> </li> </ul> </li> </ul> <div class="admonition note"> <p class=admonition-title>Code for reduce on loss plateau learning rate decay of factor 0.1 and 0 patience</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>

<span class=c1># Set seed</span>
<span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Where to add a new import</span>
<span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>ReduceLROnPlateau</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>6000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>FeedforwardNeuralNetModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Linear function</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> 
        <span class=c1># Non-linearity</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
        <span class=c1># Linear function (readout)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Linear function</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=c1># Linear function (readout)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>


<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>nesterov</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=c1># lr = lr * factor </span>
<span class=c1># mode=&#39;max&#39;: look for the maximum validation accuracy to track</span>
<span class=c1># patience: number of epochs - 1 where loss plateaus before decreasing LR</span>
        <span class=c1># patience = 0, after 1 bad epoch, reduce LR</span>
<span class=c1># factor = decaying factor</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>ReduceLROnPlateau</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;max&#39;</span><span class=p>,</span> <span class=n>factor</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>patience</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images as Variable</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1># Total correct predictions</span>
                <span class=c1># Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=c1># print(&#39;Iteration: {}. Loss: {}. Accuracy: {}&#39;.format(iter, loss.data[0], accuracy))</span>

    <span class=c1># Decay Learning Rate, pass validation accuracy for tracking at every epoch</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch </span><span class=si>{}</span><span class=s1> completed&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>epoch</span><span class=p>))</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;-&#39;</span><span class=o>*</span><span class=mi>20</span><span class=p>)</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>accuracy</span><span class=p>)</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>Epoch</span> <span class=mi>0</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.17087846994400024</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>96.26</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>1</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.11688263714313507</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>96.96</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>2</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.035437121987342834</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>96.78</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span>     <span class=mi>2</span><span class=p>:</span> <span class=n>reducing</span> <span class=n>learning</span> <span class=n>rate</span> <span class=n>of</span> <span class=n>group</span> <span class=mi>0</span> <span class=n>to</span> <span class=mf>1.0000e-02</span><span class=o>.</span>
<span class=n>Epoch</span> <span class=mi>3</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.0324370414018631</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>97.7</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>4</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.022194599732756615</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>98.02</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>5</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.007145566865801811</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>98.03</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>6</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.01673538237810135</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>98.05</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>7</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.025424446910619736</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>98.01</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span>     <span class=mi>7</span><span class=p>:</span> <span class=n>reducing</span> <span class=n>learning</span> <span class=n>rate</span> <span class=n>of</span> <span class=n>group</span> <span class=mi>0</span> <span class=n>to</span> <span class=mf>1.0000e-03</span><span class=o>.</span>
<span class=n>Epoch</span> <span class=mi>8</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.014696130529046059</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>98.05</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span>     <span class=mi>8</span><span class=p>:</span> <span class=n>reducing</span> <span class=n>learning</span> <span class=n>rate</span> <span class=n>of</span> <span class=n>group</span> <span class=mi>0</span> <span class=n>to</span> <span class=mf>1.0000e-04</span><span class=o>.</span>
<span class=n>Epoch</span> <span class=mi>9</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.00573748117312789</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>98.04</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span>     <span class=mi>9</span><span class=p>:</span> <span class=n>reducing</span> <span class=n>learning</span> <span class=n>rate</span> <span class=n>of</span> <span class=n>group</span> <span class=mi>0</span> <span class=n>to</span> <span class=mf>1.0000e-05</span><span class=o>.</span>
</code></pre></div> <h4 id=reduce-on-loss-plateau-decay-patience0-factor05>Reduce on Loss Plateau Decay, Patience=0, Factor=0.5<a class=headerlink href=#reduce-on-loss-plateau-decay-patience0-factor05 title="Permanent link">&para;</a></h4> <ul> <li>Reduce learning rate whenever loss plateaus<ul> <li>Patience: number of epochs with no improvement after which learning rate will be reduced<ul> <li>Patience = 0</li> </ul> </li> <li>Factor: multiplier to decrease learning rate, <span><span class=MathJax_Preview>lr = lr*factor = \gamma</span><script type=math/tex>lr = lr*factor = \gamma</script></span><ul> <li>Factor = 0.5</li> </ul> </li> </ul> </li> <li>Optimization Algorithm 4: SGD Nesterov<ul> <li>Modification of SGD Momentum <ul> <li><span><span class=MathJax_Preview>v_t = \gamma v_{t-1} + \eta \cdot \nabla J(\theta - \gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})</span><script type=math/tex>v_t = \gamma v_{t-1} + \eta \cdot  \nabla J(\theta - \gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})</script></span></li> <li><span><span class=MathJax_Preview>\theta = \theta - v_t</span><script type=math/tex>\theta = \theta - v_t</script></span></li> </ul> </li> </ul> </li> </ul> <div class="admonition note"> <p class=admonition-title>Code for reduce on loss plateau learning rate decay with factor 0.5 and 0 patience</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>

<span class=c1># Set seed</span>
<span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Where to add a new import</span>
<span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>ReduceLROnPlateau</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>6000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>FeedforwardNeuralNetModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Linear function</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> 
        <span class=c1># Non-linearity</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
        <span class=c1># Linear function (readout)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Linear function</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=c1># Linear function (readout)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>


<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>nesterov</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=c1># lr = lr * factor </span>
<span class=c1># mode=&#39;max&#39;: look for the maximum validation accuracy to track</span>
<span class=c1># patience: number of epochs - 1 where loss plateaus before decreasing LR</span>
        <span class=c1># patience = 0, after 1 bad epoch, reduce LR</span>
<span class=c1># factor = decaying factor</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>ReduceLROnPlateau</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;max&#39;</span><span class=p>,</span> <span class=n>factor</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>patience</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images as Variable</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1># Total correct predictions</span>
                 <span class=c1># Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=c1># print(&#39;Iteration: {}. Loss: {}. Accuracy: {}&#39;.format(iter, loss.data[0], accuracy))</span>

    <span class=c1># Decay Learning Rate, pass validation accuracy for tracking at every epoch</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch </span><span class=si>{}</span><span class=s1> completed&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>epoch</span><span class=p>))</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;-&#39;</span><span class=o>*</span><span class=mi>20</span><span class=p>)</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>accuracy</span><span class=p>)</span>
</code></pre></div> </div> <div class=highlight><pre><span></span><code><span class=n>Epoch</span> <span class=mi>0</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.17087846994400024</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>96.26</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>1</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.11688263714313507</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>96.96</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>2</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.035437121987342834</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>96.78</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span>     <span class=mi>2</span><span class=p>:</span> <span class=n>reducing</span> <span class=n>learning</span> <span class=n>rate</span> <span class=n>of</span> <span class=n>group</span> <span class=mi>0</span> <span class=n>to</span> <span class=mf>5.0000e-02</span><span class=o>.</span>
<span class=n>Epoch</span> <span class=mi>3</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.04893001914024353</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>97.62</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>4</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.020584167912602425</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>97.86</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>5</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.006022400688380003</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>97.95</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>6</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.028374142944812775</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>97.87</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span>     <span class=mi>6</span><span class=p>:</span> <span class=n>reducing</span> <span class=n>learning</span> <span class=n>rate</span> <span class=n>of</span> <span class=n>group</span> <span class=mi>0</span> <span class=n>to</span> <span class=mf>2.5000e-02</span><span class=o>.</span>
<span class=n>Epoch</span> <span class=mi>7</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.013204765506088734</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>98.0</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span> <span class=mi>8</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.010137186385691166</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>97.95</span>
<span class=o>--------------------</span>
<span class=n>Epoch</span>     <span class=mi>8</span><span class=p>:</span> <span class=n>reducing</span> <span class=n>learning</span> <span class=n>rate</span> <span class=n>of</span> <span class=n>group</span> <span class=mi>0</span> <span class=n>to</span> <span class=mf>1.2500e-02</span><span class=o>.</span>
<span class=n>Epoch</span> <span class=mi>9</span> <span class=n>completed</span>
<span class=n>Loss</span><span class=p>:</span> <span class=mf>0.0035198689438402653</span><span class=o>.</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>98.01</span>
<span class=o>--------------------</span>
</code></pre></div> <h2 id=pointers-on-reduce-on-loss-pleateau-decay>Pointers on Reduce on Loss Pleateau Decay<a class=headerlink href=#pointers-on-reduce-on-loss-pleateau-decay title="Permanent link">&para;</a></h2> <ul> <li>In these examples, we used patience=1 because we are running few epochs<ul> <li>You should look at a larger patience such as 5 if for example you ran 500 epochs. </li> </ul> </li> <li>You should experiment with 2 properties <ul> <li>Patience</li> <li>Decay factor </li> </ul> </li> </ul> <h2 id=summary>Summary<a class=headerlink href=#summary title="Permanent link">&para;</a></h2> <p>We've learnt...</p> <div class="admonition success"> <p class=admonition-title>Success</p> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Learning Rate Intuition<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Update parameters so model can churn output closer to labels</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Gradual parameter updates</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Learning Rate Pointers<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> If we set <span><span class=MathJax_Preview>\eta</span><script type=math/tex>\eta</script></span> to be a <strong>large value</strong> <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> learn too much (rapid learning)</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> If we set <span><span class=MathJax_Preview>\eta</span><script type=math/tex>\eta</script></span> to be a <strong>small value</strong> <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> learn too little (slow learning)</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Learning Rate Schedules<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Step-wise Decay</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Reduce on Loss Plateau Decay</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Step-wise Decay<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Every 1 epoch</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Every 2 epoch</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Every 1 epoch, larger gamma</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Step-wise Decay Pointers<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Decay LR gradually<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Larger <span><span class=MathJax_Preview>\gamma</span><script type=math/tex>\gamma</script></span></li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Larger interval of decay (increase epoch)</li> </ul> </li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Reduce on Loss Plateau Decay<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Patience=0, Factor=1</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Patience=0, Factor=0.5</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Pointers on Reduce on Loss Plateau Decay<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Larger patience with more epochs</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> 2 hyperparameters to experiment<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Patience</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Decay factor</li> </ul> </li> </ul> </li> </ul> </div> <h2 id=citation>Citation<a class=headerlink href=#citation title="Permanent link">&para;</a></h2> <p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p><a href=https://zenodo.org/badge/latestdoi/139945544><img alt=DOI src=https://zenodo.org/badge/139945544.svg></a> </p> </article> </div> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 Disponha </div> Todo o conteúdo deste site está publicado sob a licença <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank rel=noopener> Creative Commons CC BY-SA 4.0 Brasil </a> </div> <div class=md-footer-social> <a href=https://github.com/andhremattos target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://facebook.com/dhematos target=_blank rel=noopener title=facebook.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg> </a> <a href=https://instagram.com/dhematos target=_blank rel=noopener title=instagram.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../../../assets/javascripts/vendor.de50e36d.min.js></script> <script src=../../../assets/javascripts/bundle.fc9c3121.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copiar para \u00e1rea de transfer\u00eancia", "clipboard.copied": "Copiado para \u00e1rea de transfer\u00eancia", "search.config.lang": "pt", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Digite para iniciar a busca", "search.result.none": "Nenhum resultado encontrado", "search.result.one": "1 resultado encontrado", "search.result.other": "# resultados encontrados"}</script> <script>
        app = initialize({
          base: "../../..",
          features: ["tabs"],
          search: Object.assign({
            worker:"../../../assets/javascripts/worker/search.a68abb33.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> <script src=../../../javascripts/config.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> </body> </html>