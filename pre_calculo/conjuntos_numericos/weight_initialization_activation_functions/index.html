<!doctype html><html lang=pt class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Plataforma para disponibilização cursos de cálculo."><link href=https:///disponha.com/pre_calculo/conjuntos_numericos/weight_initialization_activation_functions/ rel=canonical><meta name=author content="Carlos André"><link rel="shortcut icon" href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.1.2, mkdocs-material-5.4.0"><title>Weight Initializations & Activation Functions - disponha</title><link rel=stylesheet href=../../../assets/stylesheets/main.545621a7.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.36d1b78f.min.css><meta name=theme-color content=#009688><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Helvetica:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Helvetica",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","None","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=teal data-md-color-accent=teal> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#weight-initializations-activation-functions class=md-skip> Ir para o conteúdo </a> </div> <!-- 
    <div data-md-component="announce">
      
    </div>
  --> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https:/disponha.com/ title=disponha class="md-header-nav__button md-logo" aria-label=disponha> <?xml version="1.0" encoding="UTF-8"?> <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><!-- Creator: CorelDRAW X7 --><svg xmlns=http://www.w3.org/2000/svg xml:space=preserve width=9000px height=9000px version=1.1 style="shape-rendering:geometricPrecision; text-rendering:geometricPrecision; image-rendering:optimizeQuality; fill-rule:evenodd; clip-rule:evenodd" viewbox="0 0 9000 9000" xmlns:xlink=http://www.w3.org/1999/xlink> <defs> <style type=text/css>
   <![CDATA[
    .str0 {stroke:#2E72A4;stroke-width:118.11}
    .fil0 {fill:#FEFEFE}
    .fil1 {fill:#2E72A4}
   ]]>
  </style> </defs> <g id=Camada_x0020_1> <metadata id=CorelCorpID_0Corel-Layer /> <g id=_1793748728448> <circle cx=4472 cy=4507 r=4281 class="fil0 str0"/> <path class="fil1 str0" d="M2045 5171c120,204 210,379 402,449 314,115 1000,-177 1339,-330 79,-35 179,-68 254,-120 41,-13 94,-46 135,-68l951 -479c363,-177 729,-363 1095,-521 186,-80 370,-180 558,-244 465,-158 1366,-413 1814,-369l1 -59c-328,-34 -853,93 -1189,181 -431,113 -680,200 -1103,395 -706,324 -1401,670 -2099,1021 -397,199 -1256,618 -1669,556 -218,-33 -320,-195 -449,-416 -47,-80 -137,-241 -177,-328 -49,-108 -114,-212 -179,-311 -379,-575 -831,-826 -1274,-1083l-119 -63 -10 46 247 149c311,184 648,381 929,716 107,128 198,250 291,408 61,104 217,424 252,470z"/> </g> </g> </svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> disponha </span> <span class="md-header-nav__topic md-ellipsis"> Weight Initializations & Activation Functions </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Buscar placeholder=Buscar autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <!-- 

<a href="https://github.com/andhremattos/" title="Ir ao repositório" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    esead
  </div>
</a>
--> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <!--
      <a href="../../.." class="md-tabs__link md-tabs__link--active">
        Home
      </a>
    --> <a href=/explore/ class="md-tabs__link md-tabs__link--active"> Explore e-books </a> </li> <!--
    <li class="md-tabs__item">
      
        <a href="../../../sobre/" class="md-tabs__link">
          Sobre
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../explore/" class="md-tabs__link">
          Explore
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../app_web_ebook/servico/" class="md-tabs__link">
          App Web ebook
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../diagramacao/servico/" class="md-tabs__link">
          Diagramação
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../ciencia_de_dados/servico/" class="md-tabs__link">
          Ciência de dados
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../aprendacalculo/" class="md-tabs__link">
          Aprenda Cálculo
        </a>
      
    </li>--> <!--
    <li class="md-tabs__item">
      
        <a href="../../../cursolatex/" class="md-tabs__link">
          Curso de LaTeX
        </a>
      
    </li>--> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https:/disponha.com/ title=disponha class="md-nav__button md-logo" aria-label=disponha> <?xml version="1.0" encoding="UTF-8"?> <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><!-- Creator: CorelDRAW X7 --><svg xmlns=http://www.w3.org/2000/svg xml:space=preserve width=9000px height=9000px version=1.1 style="shape-rendering:geometricPrecision; text-rendering:geometricPrecision; image-rendering:optimizeQuality; fill-rule:evenodd; clip-rule:evenodd" viewbox="0 0 9000 9000" xmlns:xlink=http://www.w3.org/1999/xlink> <defs> <style type=text/css>
   <![CDATA[
    .str0 {stroke:#2E72A4;stroke-width:118.11}
    .fil0 {fill:#FEFEFE}
    .fil1 {fill:#2E72A4}
   ]]>
  </style> </defs> <g id=Camada_x0020_1> <metadata id=CorelCorpID_0Corel-Layer /> <g id=_1793748728448> <circle cx=4472 cy=4507 r=4281 class="fil0 str0"/> <path class="fil1 str0" d="M2045 5171c120,204 210,379 402,449 314,115 1000,-177 1339,-330 79,-35 179,-68 254,-120 41,-13 94,-46 135,-68l951 -479c363,-177 729,-363 1095,-521 186,-80 370,-180 558,-244 465,-158 1366,-413 1814,-369l1 -59c-328,-34 -853,93 -1189,181 -431,113 -680,200 -1103,395 -706,324 -1401,670 -2099,1021 -397,199 -1256,618 -1669,556 -218,-33 -320,-195 -449,-416 -47,-80 -137,-241 -177,-328 -49,-108 -114,-212 -179,-311 -379,-575 -831,-826 -1274,-1083l-119 -63 -10 46 247 149c311,184 648,381 929,716 107,128 198,250 291,408 61,104 217,424 252,470z"/> </g> </g> </svg> </a> disponha </label> <div class=md-nav__source> <!-- 

<a href="https://github.com/andhremattos/" title="Ir ao repositório" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    esead
  </div>
</a>
--> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. title=Home class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2> <label class=md-nav__link for=nav-2> Sobre <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Sobre data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Sobre </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../sobre/ title=Home class=md-nav__link> Home </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Explore <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Explore data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Explore </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../explore/ title=E-books class=md-nav__link> E-books </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../apoie/ title=Apoie class=md-nav__link> Apoie </a> </li> <li class=md-nav__item> <a href=../../../consultoria/ title=Consultoria class=md-nav__link> Consultoria </a> </li> <li class=md-nav__item> <a href=../../../politica_privacidade/ title=Privacidade class=md-nav__link> Privacidade </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7 type=checkbox id=nav-7> <label class=md-nav__link for=nav-7> App Web ebook <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="App Web ebook" data-md-level=1> <label class=md-nav__title for=nav-7> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> App Web ebook </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../app_web_ebook/servico/ title=Serviço class=md-nav__link> Serviço </a> </li> <li class=md-nav__item> <a href=../../../app_web_ebook/amostra/ title="App Web ebook - amostra" class=md-nav__link> App Web ebook - amostra </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-8 type=checkbox id=nav-8> <label class=md-nav__link for=nav-8> Diagramação <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Diagramação data-md-level=1> <label class=md-nav__title for=nav-8> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Diagramação </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../diagramacao/servico/ title=Serviço class=md-nav__link> Serviço </a> </li> <li class=md-nav__item> <a href=../../../diagramacao/amostra-diagramacao/ title=Amostra class=md-nav__link> Amostra </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-9 type=checkbox id=nav-9> <label class=md-nav__link for=nav-9> Ciência de dados <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Ciência de dados" data-md-level=1> <label class=md-nav__title for=nav-9> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Ciência de dados </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../ciencia_de_dados/servico/ title=Serviço class=md-nav__link> Serviço </a> </li> <li class=md-nav__item> <a href=../../../ciencia_de_dados/amostra-1/ title="Amostra 1" class=md-nav__link> Amostra 1 </a> </li> <li class=md-nav__item> <a href=../../../ciencia_de_dados/amostra-2/ title="Amostra 2" class=md-nav__link> Amostra 2 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-10 type=checkbox id=nav-10> <label class=md-nav__link for=nav-10> Aprenda Cálculo <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Aprenda Cálculo" data-md-level=1> <label class=md-nav__title for=nav-10> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Aprenda Cálculo </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../aprendacalculo/ title=Bem-Vindo(a)s class=md-nav__link> Bem-Vindo(a)s </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/pre_calculo/ title=Pré-Cálculo class=md-nav__link> Pré-Cálculo </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_i/ title="Cálculo I" class=md-nav__link> Cálculo I </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_ii/ title="Cálculo II" class=md-nav__link> Cálculo II </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_iii/ title="Cálculo III" class=md-nav__link> Cálculo III </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_iv/ title="Cálculo IV" class=md-nav__link> Cálculo IV </a> </li> <li class=md-nav__item> <a href=../../../aprendacalculo/calculo_numerico/ title="C. Numérico" class=md-nav__link> C. Numérico </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-11 type=checkbox id=nav-11> <label class=md-nav__link for=nav-11> Curso de LaTeX <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Curso de LaTeX" data-md-level=1> <label class=md-nav__title for=nav-11> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Curso de LaTeX </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../cursolatex/ title=Bem-Vindo(a)s class=md-nav__link> Bem-Vindo(a)s </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/programas/ title=Programas class=md-nav__link> Programas </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/latex_online/ title="LaTeX online" class=md-nav__link> LaTeX online </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_1/ title="Parte 1" class=md-nav__link> Parte 1 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_2/ title="Parte 2" class=md-nav__link> Parte 2 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_3/ title="Parte 3" class=md-nav__link> Parte 3 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_4/ title="Parte 4" class=md-nav__link> Parte 4 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_5/ title="Parte 5" class=md-nav__link> Parte 5 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_6/ title="Parte 6" class=md-nav__link> Parte 6 </a> </li> <li class=md-nav__item> <a href=../../../cursolatex/parte_7/ title="Parte 7" class=md-nav__link> Parte 7 </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=Índice> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Índice </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#recap-of-logistic-regression class=md-nav__link> Recap of Logistic Regression </a> </li> <li class=md-nav__item> <a href=#recap-of-feedforward-neural-network-activation-function class=md-nav__link> Recap of Feedforward Neural Network Activation Function </a> <nav class=md-nav aria-label="Recap of Feedforward Neural Network Activation Function"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sigmoid-logistic class=md-nav__link> Sigmoid (Logistic) </a> </li> <li class=md-nav__item> <a href=#tanh class=md-nav__link> Tanh </a> </li> <li class=md-nav__item> <a href=#relus class=md-nav__link> ReLUs </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#why-do-we-need-weight-initializations-or-new-activation-functions class=md-nav__link> Why do we need weight initializations or new activation functions? </a> <nav class=md-nav aria-label="Why do we need weight initializations or new activation functions?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#case-1-sigmoidtanh class=md-nav__link> Case 1: Sigmoid/Tanh </a> </li> <li class=md-nav__item> <a href=#case-2-relu class=md-nav__link> Case 2: ReLU </a> </li> <li class=md-nav__item> <a href=#case-3-leaky-relu class=md-nav__link> Case 3: Leaky ReLU </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#summary-of-weight-initialization-solutions-to-activations class=md-nav__link> Summary of weight initialization solutions to activations </a> </li> <li class=md-nav__item> <a href=#types-of-weight-intializations class=md-nav__link> Types of weight intializations </a> <nav class=md-nav aria-label="Types of weight intializations"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#zero-initialization-set-all-weights-to-0 class=md-nav__link> Zero Initialization: set all weights to 0 </a> </li> <li class=md-nav__item> <a href=#normal-initialization-set-all-weights-to-random-small-numbers class=md-nav__link> Normal Initialization: set all weights to random small numbers </a> </li> <li class=md-nav__item> <a href=#lecun-initialization-normalize-variance class=md-nav__link> Lecun Initialization: normalize variance </a> <nav class=md-nav aria-label="Lecun Initialization: normalize variance"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#equations-for-lecun-initialization class=md-nav__link> Equations for Lecun Initialization </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#improvements-to-lecun-intialization class=md-nav__link> Improvements to Lecun Intialization </a> </li> <li class=md-nav__item> <a href=#summary-of-weight-initializations class=md-nav__link> Summary of weight initializations </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#weight-initializations-with-pytorch class=md-nav__link> Weight Initializations with PyTorch </a> <nav class=md-nav aria-label="Weight Initializations with PyTorch"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#normal-initialization-tanh-activation class=md-nav__link> Normal Initialization: Tanh Activation </a> </li> <li class=md-nav__item> <a href=#lecun-initialization-tanh-activation class=md-nav__link> Lecun Initialization: Tanh Activation </a> </li> <li class=md-nav__item> <a href=#xavier-initialization-tanh-activation class=md-nav__link> Xavier Initialization: Tanh Activation </a> </li> <li class=md-nav__item> <a href=#xavier-initialization-relu-activation class=md-nav__link> Xavier Initialization: ReLU Activation </a> </li> <li class=md-nav__item> <a href=#he-initialization-relu-activation class=md-nav__link> He Initialization: ReLU Activation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#initialization-performance class=md-nav__link> Initialization Performance </a> </li> <li class=md-nav__item> <a href=#summary class=md-nav__link> Summary </a> </li> <li class=md-nav__item> <a href=#citation class=md-nav__link> Citation </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=weight-initializations-activation-functions>Weight Initializations &amp; Activation Functions<a class=headerlink href=#weight-initializations-activation-functions title="Permanent link">&para;</a></h1> <div class="admonition tip"> <p class=admonition-title>Run Jupyter Notebook</p> <p>You can run the code for this section in this <a href=https://github.com/ritchieng/deep-learning-wizard/blob/master/docs/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions.ipynb>jupyter notebook link</a>.</p> </div> <h2 id=recap-of-logistic-regression>Recap of Logistic Regression<a class=headerlink href=#recap-of-logistic-regression title="Permanent link">&para;</a></h2> <p><img alt src=../images/cross_entropy_final_4.png></p> <h2 id=recap-of-feedforward-neural-network-activation-function>Recap of Feedforward Neural Network Activation Function<a class=headerlink href=#recap-of-feedforward-neural-network-activation-function title="Permanent link">&para;</a></h2> <p><img alt src=../images/logistic_regression_comparison_nn5.png></p> <h4 id=sigmoid-logistic>Sigmoid (Logistic)<a class=headerlink href=#sigmoid-logistic title="Permanent link">&para;</a></h4> <ul> <li><span><span class=MathJax_Preview>\sigma(x) = \frac{1}{1 + e^{-x}}</span><script type=math/tex>\sigma(x) = \frac{1}{1 + e^{-x}}</script></span></li> <li>Input number <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> [0, 1]<ul> <li>Large negative number <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> 0</li> <li>Large positive number <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> 1</li> </ul> </li> <li>Cons: <ol> <li>Activation saturates at 0 or 1 with <strong>gradients <span><span class=MathJax_Preview>\approx</span><script type=math/tex>\approx</script></span> 0</strong><ul> <li>No signal to update weights <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> <strong>cannot learn</strong></li> <li>Solution: Have to carefully initialize weights to prevent this</li> </ul> </li> </ol> </li> </ul> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=o>%</span><span class=n>matplotlib</span> <span class=n>inline</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>

<span class=k>def</span> <span class=nf>sigmoid</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
    <span class=n>a</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=k>for</span> <span class=n>item</span> <span class=ow>in</span> <span class=n>x</span><span class=p>:</span>
        <span class=n>a</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=mi>1</span><span class=o>/</span><span class=p>(</span><span class=mi>1</span><span class=o>+</span><span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>item</span><span class=p>)))</span>
    <span class=k>return</span> <span class=n>a</span>

<span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=o>-</span><span class=mf>10.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>)</span>
<span class=n>sig</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>style</span><span class=o>.</span><span class=n>use</span><span class=p>(</span><span class=s1>&#39;ggplot&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>sig</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mf>3.0</span><span class=p>)</span>
</code></pre></div> <p><img alt=png src=../weight_initialization_activation_functions_files/weight_initialization_activation_functions_1_1.png></p> <h4 id=tanh>Tanh<a class=headerlink href=#tanh title="Permanent link">&para;</a></h4> <ul> <li><span><span class=MathJax_Preview>\tanh(x) = 2 \sigma(2x) -1</span><script type=math/tex>\tanh(x) = 2 \sigma(2x) -1</script></span><ul> <li>A scaled sigmoid function</li> </ul> </li> <li>Input number <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> [-1, 1]</li> <li>Cons: <ol> <li>Activation saturates at 0 or 1 with <strong>gradients <span><span class=MathJax_Preview>\approx</span><script type=math/tex>\approx</script></span> 0</strong><ul> <li>No signal to update weights <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> <strong>cannot learn</strong></li> <li><strong>Solution</strong>: Have to carefully initialize weights to prevent this</li> </ul> </li> </ol> </li> </ul> <div class=highlight><pre><span></span><code><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=o>-</span><span class=mf>10.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>)</span>
<span class=n>tanh</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>x</span><span class=p>)))</span> <span class=o>-</span> <span class=mi>1</span>

<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>tanh</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mf>3.0</span><span class=p>)</span>
</code></pre></div> <p><img alt=png src=../weight_initialization_activation_functions_files/weight_initialization_activation_functions_3_1.png></p> <h4 id=relus>ReLUs<a class=headerlink href=#relus title="Permanent link">&para;</a></h4> <ul> <li><span><span class=MathJax_Preview>f(x) = \max(0, x)</span><script type=math/tex>f(x) = \max(0, x)</script></span></li> <li>Pros:<ol> <li>Accelerates convergence <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> <strong>train faster</strong></li> <li><strong>Less computationally expensive operation</strong> compared to Sigmoid/Tanh exponentials</li> </ol> </li> <li>Cons:<ol> <li>Many ReLU units "die" <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> <strong>gradients = 0</strong> forever<ul> <li><strong>Solution</strong>: careful learning rate and weight initialization choice</li> </ul> </li> </ol> </li> </ul> <div class=highlight><pre><span></span><code><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=o>-</span><span class=mf>10.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>)</span>
<span class=n>relu</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>relu</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mf>3.0</span><span class=p>)</span>
</code></pre></div> <p><img alt=png src=../weight_initialization_activation_functions_files/weight_initialization_activation_functions_5_1.png></p> <h2 id=why-do-we-need-weight-initializations-or-new-activation-functions>Why do we need weight initializations or new activation functions?<a class=headerlink href=#why-do-we-need-weight-initializations-or-new-activation-functions title="Permanent link">&para;</a></h2> <ul> <li><strong>To prevent vanishing/exploding gradients</strong></li> </ul> <h3 id=case-1-sigmoidtanh>Case 1: Sigmoid/Tanh<a class=headerlink href=#case-1-sigmoidtanh title="Permanent link">&para;</a></h3> <ul> <li><strong>Problem</strong><ul> <li>If variance of input too large: gradients = 0 (vanishing gradients)</li> <li>If variance of input too small: linear <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> gradients = constant value</li> </ul> </li> <li><strong>Solutions</strong><ul> <li>Want a constant variance of input to achieve non-linearity <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> unique gradients for unique updates<ul> <li>Xavier Initialization (good constant variance for Sigmoid/Tanh)</li> <li>ReLU or Leaky ReLU</li> </ul> </li> </ul> </li> </ul> <h3 id=case-2-relu>Case 2: ReLU<a class=headerlink href=#case-2-relu title="Permanent link">&para;</a></h3> <ul> <li><strong>Solution to Case 1</strong><ul> <li>Regardless of variance of input: gradients = 0 or 1 </li> </ul> </li> <li><strong>Problem</strong><ul> <li>But those with 0: no updates ("dead ReLU units") </li> <li>Has unlimited output size with input &gt; 0 (explodes gradients subsequently)</li> </ul> </li> <li><strong>Solutions</strong><ul> <li>He Initialization (good constant variance)</li> <li>Leaky ReLU</li> </ul> </li> </ul> <h3 id=case-3-leaky-relu>Case 3: Leaky ReLU<a class=headerlink href=#case-3-leaky-relu title="Permanent link">&para;</a></h3> <ul> <li><strong>Solution to Case 2</strong><ul> <li>Solves the 0 signal issue when input &lt; 0 <img alt src=../images/leaky_relu_compare2.png></li> </ul> </li> <li><strong>Problem</strong><ul> <li>Has unlimited output size with input &gt; 0 (explodes)</li> </ul> </li> <li><strong>Solution</strong><ul> <li>He Initialization (good constant variance)</li> </ul> </li> </ul> <h2 id=summary-of-weight-initialization-solutions-to-activations>Summary of weight initialization solutions to activations<a class=headerlink href=#summary-of-weight-initialization-solutions-to-activations title="Permanent link">&para;</a></h2> <ul> <li>Tanh/Sigmoid vanishing gradients can be solved with Xavier initialization<ul> <li>Good range of constant variance</li> </ul> </li> <li>ReLU/Leaky ReLU exploding gradients can be solved with He initialization<ul> <li>Good range of constant variance</li> </ul> </li> </ul> <h2 id=types-of-weight-intializations>Types of weight intializations<a class=headerlink href=#types-of-weight-intializations title="Permanent link">&para;</a></h2> <h4 id=zero-initialization-set-all-weights-to-0>Zero Initialization: set all weights to 0<a class=headerlink href=#zero-initialization-set-all-weights-to-0 title="Permanent link">&para;</a></h4> <ul> <li>Every neuron in the network computes the same output <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> computes the same gradient <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> same parameter updates </li> </ul> <h4 id=normal-initialization-set-all-weights-to-random-small-numbers>Normal Initialization: set all weights to random small numbers<a class=headerlink href=#normal-initialization-set-all-weights-to-random-small-numbers title="Permanent link">&para;</a></h4> <ul> <li>Every neuron in the network computes different output <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> computes different gradient <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> different parameter updates </li> <li>"Symmetry breaking" </li> <li>Problem: variance that grows with the number of inputs</li> </ul> <h4 id=lecun-initialization-normalize-variance>Lecun Initialization: normalize variance<a class=headerlink href=#lecun-initialization-normalize-variance title="Permanent link">&para;</a></h4> <ul> <li>Solves growing variance with the number of inputs <span><span class=MathJax_Preview>\rightarrow</span><script type=math/tex>\rightarrow</script></span> constant variance </li> <li>Look at a simple feedforward neural network <img alt src=../images/nn2.png></li> </ul> <h5 id=equations-for-lecun-initialization>Equations for Lecun Initialization<a class=headerlink href=#equations-for-lecun-initialization title="Permanent link">&para;</a></h5> <ul> <li><span><span class=MathJax_Preview>Y = AX + B</span><script type=math/tex>Y = AX + B</script></span></li> <li><span><span class=MathJax_Preview>y = a_1x_1 + a_2x_2 + \cdot + a_n x_n + b</span><script type=math/tex>y = a_1x_1 + a_2x_2 + \cdot + a_n x_n + b</script></span></li> <li><span><span class=MathJax_Preview>Var(y) = Var(a_1x_1 + a_2x_2 + \cdot + a_n x_n + b)</span><script type=math/tex>Var(y) = Var(a_1x_1 + a_2x_2 + \cdot + a_n x_n + b)</script></span></li> <li><span><span class=MathJax_Preview>Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i)</span><script type=math/tex>Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i)</script></span><ul> <li>General term, you might be more familiar with the following<ul> <li><span><span class=MathJax_Preview>Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y)</span><script type=math/tex>Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y)</script></span></li> </ul> </li> <li><span><span class=MathJax_Preview>E(x_i)</span><script type=math/tex>E(x_i)</script></span>: expectation/mean of <span><span class=MathJax_Preview>x_i</span><script type=math/tex>x_i</script></span></li> <li><span><span class=MathJax_Preview>E(a_i)</span><script type=math/tex>E(a_i)</script></span>: expectation/mean of <span><span class=MathJax_Preview>a_i</span><script type=math/tex>a_i</script></span></li> </ul> </li> <li>Assuming inputs/weights drawn i.i.d. with Gaussian distribution of mean=0<ul> <li><span><span class=MathJax_Preview>E(x_i) = E(a_i) = 0</span><script type=math/tex>E(x_i) = E(a_i) = 0</script></span></li> <li><span><span class=MathJax_Preview>Var(a_i x_i) = Var(a_i)Var(x_i)</span><script type=math/tex>Var(a_i x_i) = Var(a_i)Var(x_i)</script></span></li> </ul> </li> <li><span><span class=MathJax_Preview>Var(y) = Var(a_1)Var(x_1) + \cdot + Var(a_n)Var(x_n)</span><script type=math/tex>Var(y) = Var(a_1)Var(x_1) + \cdot + Var(a_n)Var(x_n)</script></span><ul> <li>Since the bias, b, is a constant, <span><span class=MathJax_Preview>Var(b) = 0</span><script type=math/tex>Var(b) = 0</script></span></li> </ul> </li> <li>Since i.i.d.<ul> <li><span><span class=MathJax_Preview>Var(y) = n \times Var(a_i)Var(x_i)</span><script type=math/tex>Var(y) = n \times Var(a_i)Var(x_i)</script></span></li> </ul> </li> <li>Since we want constant variance where <span><span class=MathJax_Preview>Var(y) = Var(x_i)</span><script type=math/tex>Var(y) = Var(x_i)</script></span><ul> <li><span><span class=MathJax_Preview>1 = nVar(a_i)</span><script type=math/tex>1 = nVar(a_i)</script></span></li> <li><span><span class=MathJax_Preview>Var(a_i) = \frac{1}{n}</span><script type=math/tex>Var(a_i) = \frac{1}{n}</script></span></li> </ul> </li> <li>This is essentially <a href=http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf>Lecun initialization, from his paper titled "Efficient Backpropagation"</a><ul> <li>We draw our weights i.i.d. with mean=0 and variance = <span><span class=MathJax_Preview>\frac{1}{n}</span><script type=math/tex>\frac{1}{n}</script></span></li> <li>Where <span><span class=MathJax_Preview>n</span><script type=math/tex>n</script></span> is the number of <strong>input units</strong> in the weight tensor</li> </ul> </li> </ul> <h4 id=improvements-to-lecun-intialization>Improvements to Lecun Intialization<a class=headerlink href=#improvements-to-lecun-intialization title="Permanent link">&para;</a></h4> <ul> <li>They are essentially slight modifications to Lecun'98 initialization</li> <li>Xavier Intialization<ul> <li>Works better for layers with Sigmoid activations </li> <li><span><span class=MathJax_Preview>var(a_i) = \frac{1}{n_{in} + n_{out}}</span><script type=math/tex>var(a_i) = \frac{1}{n_{in} + n_{out}}</script></span><ul> <li>Where <span><span class=MathJax_Preview>n_{in}</span><script type=math/tex>n_{in}</script></span> and <span><span class=MathJax_Preview>n_{out}</span><script type=math/tex>n_{out}</script></span> are the number of input and output units in the weight tensor respectively</li> </ul> </li> </ul> </li> <li>Kaiming Initialization<ul> <li>Works better for layers with ReLU or LeakyReLU activations </li> <li><span><span class=MathJax_Preview>var(a_i) = \frac{2}{n_{in}}</span><script type=math/tex>var(a_i) = \frac{2}{n_{in}}</script></span></li> </ul> </li> </ul> <h4 id=summary-of-weight-initializations>Summary of weight initializations<a class=headerlink href=#summary-of-weight-initializations title="Permanent link">&para;</a></h4> <ul> <li>Normal Distribution</li> <li>Lecun Normal Distribution</li> <li>Xavier (Glorot) Normal Distribution </li> <li>Kaiming (He) Normal Distribution</li> </ul> <h2 id=weight-initializations-with-pytorch>Weight Initializations with PyTorch<a class=headerlink href=#weight-initializations-with-pytorch title="Permanent link">&para;</a></h2> <h3 id=normal-initialization-tanh-activation>Normal Initialization: Tanh Activation<a class=headerlink href=#normal-initialization-tanh-activation title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>
<span class=kn>from</span> <span class=nn>torch.autograd</span> <span class=kn>import</span> <span class=n>Variable</span>

<span class=c1># Set seed</span>
<span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Scheduler import</span>
<span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>StepLR</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>3000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>FeedforwardNeuralNetModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Linear function</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> 
        <span class=c1># Linear weight, W,  Y = WX + B</span>
        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=o>.</span><span class=n>weight</span><span class=p>,</span> <span class=n>mean</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>std</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>tanh</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Tanh</span><span class=p>()</span>
        <span class=c1># Linear function (readout)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  
        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=o>.</span><span class=n>weight</span><span class=p>,</span> <span class=n>mean</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>std</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Linear function</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=c1># Linear function (readout)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>nesterov</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=c1># step_size: at how many multiples of epoch you decay</span>
<span class=c1># step_size = 1, after every 2 epoch, new_lr = lr*gamma </span>
<span class=c1># step_size = 2, after every 2 epoch, new_lr = lr*gamma </span>

<span class=c1># gamma = decaying factor</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.96</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 8: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=c1># Decay Learning Rate</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
    <span class=c1># Print Learning Rate</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch:&#39;</span><span class=p>,</span> <span class=n>epoch</span><span class=p>,</span><span class=s1>&#39;LR:&#39;</span><span class=p>,</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>get_lr</span><span class=p>())</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images as tensors with gradient accumulation abilities</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

               <span class=c1># Total correct predictions</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>)</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span> <span class=o>==</span> <span class=n>labels</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>))</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mf>100.</span> <span class=o>*</span> <span class=n>correct</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Iteration: </span><span class=si>{}</span><span class=s1>. Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
</code></pre></div> <div class=codehilite><pre><span></span><code>Epoch: 0 LR: [0.1]
Iteration: 500. Loss: 0.5192779302597046. Accuracy: 87.9
Epoch: 1 LR: [0.096]
Iteration: 1000. Loss: 0.4060308337211609. Accuracy: 90.15
Epoch: 2 LR: [0.09216]
Iteration: 1500. Loss: 0.2880493104457855. Accuracy: 90.71
Epoch: 3 LR: [0.08847359999999999]
Iteration: 2000. Loss: 0.23173095285892487. Accuracy: 91.99
Epoch: 4 LR: [0.084934656]
Iteration: 2500. Loss: 0.23814399540424347. Accuracy: 92.32
Iteration: 3000. Loss: 0.19513173401355743. Accuracy: 92.55
</code></pre></div> <h3 id=lecun-initialization-tanh-activation>Lecun Initialization: Tanh Activation<a class=headerlink href=#lecun-initialization-tanh-activation title="Permanent link">&para;</a></h3> <ul> <li>By default, PyTorch uses Lecun initialization, so nothing new has to be done here compared to using Normal, Xavier or Kaiming initialization.</li> </ul> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>
<span class=kn>from</span> <span class=nn>torch.autograd</span> <span class=kn>import</span> <span class=n>Variable</span>

<span class=c1># Set seed</span>
<span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Scheduler import</span>
<span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>StepLR</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>3000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>FeedforwardNeuralNetModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Linear function</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> 
        <span class=c1># Non-linearity</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>tanh</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Tanh</span><span class=p>()</span>
        <span class=c1># Linear function (readout)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Linear function</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=c1># Linear function (readout)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>nesterov</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=c1># step_size: at how many multiples of epoch you decay</span>
<span class=c1># step_size = 1, after every 2 epoch, new_lr = lr*gamma </span>
<span class=c1># step_size = 2, after every 2 epoch, new_lr = lr*gamma </span>

<span class=c1># gamma = decaying factor</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.96</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 8: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=c1># Decay Learning Rate</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
    <span class=c1># Print Learning Rate</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch:&#39;</span><span class=p>,</span> <span class=n>epoch</span><span class=p>,</span><span class=s1>&#39;LR:&#39;</span><span class=p>,</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>get_lr</span><span class=p>())</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images as tensors with gradient accumulation abilities</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1># Total correct predictions</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>)</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span> <span class=o>==</span> <span class=n>labels</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>))</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mf>100.</span> <span class=o>*</span> <span class=n>correct</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Iteration: </span><span class=si>{}</span><span class=s1>. Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
</code></pre></div> <div class=codehilite><pre><span></span><code>Epoch: 0 LR: [0.1]
Iteration: 500. Loss: 0.20123475790023804. Accuracy: 95.63
Epoch: 1 LR: [0.096]
Iteration: 1000. Loss: 0.10885068774223328. Accuracy: 96.48
Epoch: 2 LR: [0.09216]
Iteration: 1500. Loss: 0.1296212077140808. Accuracy: 97.22
Epoch: 3 LR: [0.08847359999999999]
Iteration: 2000. Loss: 0.05178885534405708. Accuracy: 97.36
Epoch: 4 LR: [0.084934656]
Iteration: 2500. Loss: 0.02619408629834652. Accuracy: 97.61
Iteration: 3000. Loss: 0.02096685953438282. Accuracy: 97.7
</code></pre></div> <h3 id=xavier-initialization-tanh-activation>Xavier Initialization: Tanh Activation<a class=headerlink href=#xavier-initialization-tanh-activation title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>
<span class=kn>from</span> <span class=nn>torch.autograd</span> <span class=kn>import</span> <span class=n>Variable</span>

<span class=c1># Set seed</span>
<span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Scheduler import</span>
<span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>StepLR</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>3000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>FeedforwardNeuralNetModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Linear function</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> 
        <span class=c1># Linear weight, W,  Y = WX + B</span>
        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>xavier_normal_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>tanh</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Tanh</span><span class=p>()</span>
        <span class=c1># Linear function (readout)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  
        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>xavier_normal_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Linear function</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=c1># Linear function (readout)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>


<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>nesterov</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=c1># step_size: at how many multiples of epoch you decay</span>
<span class=c1># step_size = 1, after every 2 epoch, new_lr = lr*gamma </span>
<span class=c1># step_size = 2, after every 2 epoch, new_lr = lr*gamma </span>

<span class=c1># gamma = decaying factor</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.96</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 8: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=c1># Decay Learning Rate</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
    <span class=c1># Print Learning Rate</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch:&#39;</span><span class=p>,</span> <span class=n>epoch</span><span class=p>,</span><span class=s1>&#39;LR:&#39;</span><span class=p>,</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>get_lr</span><span class=p>())</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images as tensors with gradient accumulation abilities</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1># Total correct predictions</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>)</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span> <span class=o>==</span> <span class=n>labels</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>))</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mf>100.</span> <span class=o>*</span> <span class=n>correct</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Iteration: </span><span class=si>{}</span><span class=s1>. Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
</code></pre></div> <div class=codehilite><pre><span></span><code>Epoch: 0 LR: [0.1]
Iteration: 500. Loss: 0.14800140261650085. Accuracy: 95.43
Epoch: 1 LR: [0.096]
Iteration: 1000. Loss: 0.17138008773326874. Accuracy: 96.58
Epoch: 2 LR: [0.09216]
Iteration: 1500. Loss: 0.07987994700670242. Accuracy: 96.95
Epoch: 3 LR: [0.08847359999999999]
Iteration: 2000. Loss: 0.07756654918193817. Accuracy: 97.23
Epoch: 4 LR: [0.084934656]
Iteration: 2500. Loss: 0.05563584715127945. Accuracy: 97.6
Iteration: 3000. Loss: 0.07122127711772919. Accuracy: 97.49
</code></pre></div> <h3 id=xavier-initialization-relu-activation>Xavier Initialization: ReLU Activation<a class=headerlink href=#xavier-initialization-relu-activation title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>
<span class=kn>from</span> <span class=nn>torch.autograd</span> <span class=kn>import</span> <span class=n>Variable</span>

<span class=c1># Set seed</span>
<span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Scheduler import</span>
<span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>StepLR</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>3000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>FeedforwardNeuralNetModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Linear function</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> 
        <span class=c1># Linear weight, W,  Y = WX + B</span>
        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>xavier_normal_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
        <span class=c1># Linear function (readout)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  
        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>xavier_normal_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Linear function</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=c1># Linear function (readout)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>


<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>nesterov</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=c1># step_size: at how many multiples of epoch you decay</span>
<span class=c1># step_size = 1, after every 2 epoch, new_lr = lr*gamma </span>
<span class=c1># step_size = 2, after every 2 epoch, new_lr = lr*gamma </span>

<span class=c1># gamma = decaying factor</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.96</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 8: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=c1># Decay Learning Rate</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
    <span class=c1># Print Learning Rate</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch:&#39;</span><span class=p>,</span> <span class=n>epoch</span><span class=p>,</span><span class=s1>&#39;LR:&#39;</span><span class=p>,</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>get_lr</span><span class=p>())</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images as tensors with gradient accumulation abilities</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1># Total correct predictions</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>)</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span> <span class=o>==</span> <span class=n>labels</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>))</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mf>100.</span> <span class=o>*</span> <span class=n>correct</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Iteration: </span><span class=si>{}</span><span class=s1>. Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
</code></pre></div> <div class=codehilite><pre><span></span><code>Epoch: 0 LR: [0.1]
Iteration: 500. Loss: 0.1245984435081482. Accuracy: 95.82
Epoch: 1 LR: [0.096]
Iteration: 1000. Loss: 0.14348150789737701. Accuracy: 96.72
Epoch: 2 LR: [0.09216]
Iteration: 1500. Loss: 0.10421314090490341. Accuracy: 97.3
Epoch: 3 LR: [0.08847359999999999]
Iteration: 2000. Loss: 0.04693891853094101. Accuracy: 97.29
Epoch: 4 LR: [0.084934656]
Iteration: 2500. Loss: 0.06869587302207947. Accuracy: 97.61
Iteration: 3000. Loss: 0.056865859776735306. Accuracy: 97.48
</code></pre></div> <h3 id=he-initialization-relu-activation>He Initialization: ReLU Activation<a class=headerlink href=#he-initialization-relu-activation title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>dsets</span>
<span class=kn>from</span> <span class=nn>torch.autograd</span> <span class=kn>import</span> <span class=n>Variable</span>

<span class=c1># Set seed</span>
<span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Scheduler import</span>
<span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>StepLR</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 1: LOADING DATASET</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                            <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                            <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
                            <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>dsets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> 
                           <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> 
                           <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>())</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 2: MAKING DATASET ITERABLE</span>
<span class=sd>&#39;&#39;&#39;</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_iters</span> <span class=o>=</span> <span class=mi>3000</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=n>n_iters</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>)</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>)</span>

<span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> 
                                           <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                           <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span> 
                                          <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
                                          <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 3: CREATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=k>class</span> <span class=nc>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>FeedforwardNeuralNetModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Linear function</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> 
        <span class=c1># Linear weight, W,  Y = WX + B</span>
        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>kaiming_normal_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
        <span class=c1># Linear function (readout)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  
        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>kaiming_normal_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Linear function</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=c1># Non-linearity</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=c1># Linear function (readout)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 4: INSTANTIATE MODEL CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>input_dim</span> <span class=o>=</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span>
<span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>FeedforwardNeuralNetModel</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 5: INSTANTIATE LOSS CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>


<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>nesterov</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=c1># step_size: at how many multiples of epoch you decay</span>
<span class=c1># step_size = 1, after every 2 epoch, new_lr = lr*gamma </span>
<span class=c1># step_size = 2, after every 2 epoch, new_lr = lr*gamma </span>

<span class=c1># gamma = decaying factor</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.96</span><span class=p>)</span>

<span class=sd>&#39;&#39;&#39;</span>
<span class=sd>STEP 8: TRAIN THE MODEL</span>
<span class=sd>&#39;&#39;&#39;</span>
<span class=nb>iter</span> <span class=o>=</span> <span class=mi>0</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=c1># Decay Learning Rate</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
    <span class=c1># Print Learning Rate</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch:&#39;</span><span class=p>,</span> <span class=n>epoch</span><span class=p>,</span><span class=s1>&#39;LR:&#39;</span><span class=p>,</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>get_lr</span><span class=p>())</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
        <span class=c1># Load images as tensors with gradient accumulation abilities</span>
        <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span>

        <span class=c1># Clear gradients w.r.t. parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Forward pass to get output/logits</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Getting gradients w.r.t. parameters</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

        <span class=c1># Updating parameters</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=nb>iter</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>if</span> <span class=nb>iter</span> <span class=o>%</span> <span class=mi>500</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=c1># Calculate Accuracy         </span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># Iterate through test dataset</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=c1># Load images to a Torch Variable</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>)</span>

                <span class=c1># Forward pass only to get logits/output</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

                <span class=c1># Get predictions from the maximum value</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

                <span class=c1># Total number of labels</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

                <span class=c1># Total correct predictions</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>)</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span> <span class=o>==</span> <span class=n>labels</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>))</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

            <span class=n>accuracy</span> <span class=o>=</span> <span class=mf>100.</span> <span class=o>*</span> <span class=n>correct</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>/</span> <span class=n>total</span>

            <span class=c1># Print Loss</span>
            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Iteration: </span><span class=si>{}</span><span class=s1>. Loss: </span><span class=si>{}</span><span class=s1>. Accuracy: </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>accuracy</span><span class=p>))</span>
</code></pre></div> <div class=codehilite><pre><span></span><code>Epoch: 0 LR: [0.1]
Iteration: 500. Loss: 0.11658752709627151. Accuracy: 95.7
Epoch: 1 LR: [0.096]
Iteration: 1000. Loss: 0.15525035560131073. Accuracy: 96.65
Epoch: 2 LR: [0.09216]
Iteration: 1500. Loss: 0.09970294684171677. Accuracy: 97.07
Epoch: 3 LR: [0.08847359999999999]
Iteration: 2000. Loss: 0.04063304886221886. Accuracy: 97.23
Epoch: 4 LR: [0.084934656]
Iteration: 2500. Loss: 0.0719323456287384. Accuracy: 97.7
Iteration: 3000. Loss: 0.04470040276646614. Accuracy: 97.39
</code></pre></div> <h2 id=initialization-performance>Initialization Performance<a class=headerlink href=#initialization-performance title="Permanent link">&para;</a></h2> <table> <thead> <tr> <th align=center>Initialization: Activation</th> <th align=center>Test Accuracy</th> </tr> </thead> <tbody> <tr> <td align=center>Normal: Tanh</td> <td align=center>92.55</td> </tr> <tr> <td align=center>Lecun: Tanh</td> <td align=center>97.7</td> </tr> <tr> <td align=center>Xavier: Tanh</td> <td align=center>97.49</td> </tr> <tr> <td align=center>Xavier: ReLU</td> <td align=center>97.48</td> </tr> <tr> <td align=center>He: ReLU</td> <td align=center>97.39</td> </tr> </tbody> </table> <div class="admonition note"> <p class=admonition-title>Interpreting the Validation Accuracy Table</p> <p>Take note that these numbers would fluctuate slightly when you change seeds. </p> <p>However, the key point here is that all the other intializations are clearly much better than a basic normal distribution.</p> <p>Whether He, Xavier, or Lecun intialization is better or any other initializations depends on the overall model's architecture (RNN/LSTM/CNN/FNN etc.), activation functions (ReLU, Sigmoid, Tanh etc.) and more.</p> <p>For example, more advanced initializations we will cover subsequently is orthogonal initialization that works better for RNN/LSTM. But due to the math involved in that, we will be covering such advanced initializations in a separate section.</p> </div> <h2 id=summary>Summary<a class=headerlink href=#summary title="Permanent link">&para;</a></h2> <p>We've learnt...</p> <div class="admonition success"> <p class=admonition-title>Success</p> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Recap of LG</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Recap of FNN</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Recap of Activation Functions<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Sigmoid (Logistic)</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Tanh</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> ReLU</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Need for Weight Initializations<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Sigmoid/Tanh: vanishing gradients<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Constant Variance initialization with Lecun or Xavier </li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> ReLU: exploding gradients with dead units<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> He Initialization</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Leaky ReLU: exploding gradients only<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> He Initialization</li> </ul> </li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Types of weight initialisations<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Zero</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Normal: growing weight variance</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Lecun: constant variance</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Xavier: constant variance for Sigmoid/Tanh</li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> Kaiming He: constant variance for ReLU activations</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox checked><span class=task-list-indicator></span></label> PyTorch implementation</li> </ul> </div> <h2 id=citation>Citation<a class=headerlink href=#citation title="Permanent link">&para;</a></h2> <p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p><a href=https://zenodo.org/badge/latestdoi/139945544><img alt=DOI src=https://zenodo.org/badge/139945544.svg></a> </p> </article> </div> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 Disponha </div> Todo o conteúdo deste site está publicado sob a licença <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank rel=noopener> Creative Commons CC BY-SA 4.0 Brasil </a> </div> <div class=md-footer-social> <a href=https://github.com/andhremattos target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://facebook.com/dhematos target=_blank rel=noopener title=facebook.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg> </a> <a href=https://instagram.com/dhematos target=_blank rel=noopener title=instagram.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../../../assets/javascripts/vendor.de50e36d.min.js></script> <script src=../../../assets/javascripts/bundle.fc9c3121.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copiar para \u00e1rea de transfer\u00eancia", "clipboard.copied": "Copiado para \u00e1rea de transfer\u00eancia", "search.config.lang": "pt", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Digite para iniciar a busca", "search.result.none": "Nenhum resultado encontrado", "search.result.one": "1 resultado encontrado", "search.result.other": "# resultados encontrados"}</script> <script>
        app = initialize({
          base: "../../..",
          features: ["tabs"],
          search: Object.assign({
            worker:"../../../assets/javascripts/worker/search.a68abb33.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> <script src=../../../javascripts/config.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> </body> </html>